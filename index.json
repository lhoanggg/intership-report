[{"uri":"https://lhoanggg.github.io/intership-report/4-eventparticipated/4.1-event1/","title":"Event 1","tags":[],"description":"","content":"Vietnam Cloud Day Event Objectives Equip executive leadership with strategic insights to navigate the Generative AI era. Disseminate best practices for establishing a unified, scalable data foundation on AWS. Present the AI-Driven Development Lifecycle (AI-DLC) and its transformative role in software engineering. Examine core security principles for Generative AI and the evolution toward autonomous AI Agents. Speakers Eric Yeo – Country GM, Vietnam, Cambodia, Laos \u0026amp; Myanmar, AWS Dr. Jens Lottner – CEO, Techcombank Ms. Trang Phung – CEO \u0026amp; Co-Founder, U2U Network Jaime Valles – VP \u0026amp; GM APJ, AWS Panelists: Jeff Johnson, Vu Van (ELSA), Nguyen Hoa Binh (Nexttech), Dieter Botha (TymeX) AWS Specialists: Kien Nguyen, Jun Kai Loke, Tamelly Lim, Binh Tran, Taiki Dang, Michael Armentano Key Highlights 1. Strategic Leadership \u0026amp; Vision Keynote Sessions: Leaders from AWS, Techcombank, and U2U Network outlined their vision for cloud and AI adoption across the region. Executive Panel: A discussion titled \u0026ldquo;Navigating the GenAI Revolution\u0026rdquo; focused on building an innovation-centric culture, aligning AI with business strategy, and managing organizational change during AI integration. 2. Data Foundation \u0026amp; Roadmap Unified Data Foundation: This session detailed the construction of a robust infrastructure for data ingestion, storage, and governance—a mandatory prerequisite for effective AI workloads. GenAI Roadmap: AWS showcased its comprehensive vision and emerging tools designed to empower organizations to drive efficiency through GenAI. 3. The Future of Software Development AI-Driven Development Lifecycle (AI-DLC): Discussions centered on a shift where AI acts not merely as an assistant but as a central collaborator. This model combines AI execution with human oversight to accelerate innovation beyond traditional methodologies. 4. Security \u0026amp; Advanced Automation Securing GenAI: Security was addressed across three layers: infrastructure, models, and applications, emphasizing encryption, zero-trust architecture, and granular access controls. AI Agents: The event concluded with a focus on the shift from basic automation to Intelligent Agents—systems capable of learning, adapting, and executing complex tasks autonomously. Key Takeaways Cultural Shift Adopting AI-DLC: Software development is evolving from human-led efforts with AI assistance to AI-centric collaboration, requiring teams to adapt their coding and testing approaches. Agents vs. Automation: There is a fundamental difference between static automation scripts and dynamic AI Agents that can make decisions based on changing inputs. Technical Pillars Data First: A unified and governed data foundation is critical for GenAI success. Security by Design: Security measures must be continuous and layered to ensure data confidentiality throughout the AI lifecycle. Applying to Work Assess Data Readiness: Evaluate the current AWS data infrastructure to ensure it meets the scalability and governance standards required for GenAI (referencing the Unified Data Foundation session). Explore AI Agents: Identify complex manual operations that are suitable for offloading to autonomous AI Agents rather than simple scripts. Adopt AI-DLC: Experiment with integrating AI tools more deeply into the development lifecycle, treating them as collaborators rather than just code completion utilities. Event Experience The summit offered a holistic perspective on the GenAI landscape, effectively balancing high-level strategy with technical depth.\nStrategic Insight: The panel featuring leaders from ELSA, Nexttech, and TymeX provided valuable real-world context on managing the cultural impact of AI. Technical Depth: The afternoon tracks were highly relevant, particularly the deep dives into AI-DLC and Securing GenAI, which directly align with our technical roadmap. Some event photos Add your event photos here\n"},{"uri":"https://lhoanggg.github.io/intership-report/","title":"Internship Report","tags":[],"description":"","content":"Internship Report Student Information: Full Name: Le Hoang Anh\nPhone Number: 0348663805\nEmail: anhlhse170327@fpt.edu.vn\nUniversity: FPT University\nMajor: Software Engineering\nClass: AWS092025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 08/09/2025 to 28/11/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "},{"uri":"https://lhoanggg.github.io/intership-report/5-workshop/5.4-s3-onprem/5.4.1-prepare/","title":"Create Lambda Functions","tags":[],"description":"","content":"Step 1: Create IAM Role for Lambda Go to IAM Console → Roles → Create role\nTrusted entity type:\nAWS service Use case: Lambda Add permissions:\nAWSLambdaVPCAccessExecutionRole AWSLambdaBasicExecutionRole Role details:\nRole name: daivietblood-lambda-role Description: IAM role for DaiVietBlood Lambda functions Click Create role\nStep 2: Create Lambda Layer for Dependencies Create a folder for dependencies: mkdir -p nodejs cd nodejs npm init -y npm install mysql2 Create zip file: cd .. zip -r mysql2-layer.zip nodejs Go to Lambda Console → Layers → Create layer\nConfigure:\nName: mysql2-layer Upload: Select mysql2-layer.zip Compatible runtimes: Node.js 18.x, Node.js 20.x Click Create\nStep 3: Create Lambda Function - Get Users Go to Lambda Console → Functions → Create function\nBasic information:\nFunction name: daivietblood-get-users Runtime: Node.js 20.x Architecture: x86_64 Execution role: Use existing role → daivietblood-lambda-role Click Create function\nAdd Layer:\nScroll to Layers → Add a layer Custom layers → Select mysql2-layer Click Add Configure VPC:\nGo to Configuration → VPC → Edit VPC: daivietblood-vpc Subnets: Select both Private Subnets Security groups: daivietblood-lambda-sg Click Save Add Environment Variables:\nGo to Configuration → Environment variables → Edit Add: DB_HOST = daivietblood-db.xxxx.ap-southeast-1.rds.amazonaws.com DB_PORT = 3306 DB_NAME = daivietblood DB_USER = admin DB_PASSWORD = YourSecurePassword123! Click Save Add code in Code tab:\nconst mysql = require(\u0026#39;mysql2/promise\u0026#39;); let connection; const getConnection = async () =\u0026gt; { if (!connection) { connection = await mysql.createConnection({ host: process.env.DB_HOST, port: process.env.DB_PORT, user: process.env.DB_USER, password: process.env.DB_PASSWORD, database: process.env.DB_NAME }); } return connection; }; exports.handler = async (event) =\u0026gt; { try { const conn = await getConnection(); const [rows] = await conn.execute(\u0026#39;SELECT * FROM users\u0026#39;); return { statusCode: 200, headers: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, body: JSON.stringify(rows) }; } catch (error) { console.error(\u0026#39;Error:\u0026#39;, error); return { statusCode: 500, headers: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, body: JSON.stringify({ error: \u0026#39;Internal server error\u0026#39; }) }; } }; Click Deploy Step 4: Create Lambda Function - Create User Create new function: daivietblood-create-user Same configuration as above (VPC, Layer, Environment Variables) Add code: const mysql = require(\u0026#39;mysql2/promise\u0026#39;); let connection; const getConnection = async () =\u0026gt; { if (!connection) { connection = await mysql.createConnection({ host: process.env.DB_HOST, port: process.env.DB_PORT, user: process.env.DB_USER, password: process.env.DB_PASSWORD, database: process.env.DB_NAME }); } return connection; }; exports.handler = async (event) =\u0026gt; { try { const body = JSON.parse(event.body); const { email, name, blood_type, phone } = body; if (!email || !name || !blood_type) { return { statusCode: 400, headers: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, body: JSON.stringify({ error: \u0026#39;Missing required fields\u0026#39; }) }; } const conn = await getConnection(); const [result] = await conn.execute( \u0026#39;INSERT INTO users (email, name, blood_type, phone) VALUES (?, ?, ?, ?)\u0026#39;, [email, name, blood_type, phone || null] ); return { statusCode: 201, headers: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, body: JSON.stringify({ id: result.insertId, email, name, blood_type, phone }) }; } catch (error) { console.error(\u0026#39;Error:\u0026#39;, error); if (error.code === \u0026#39;ER_DUP_ENTRY\u0026#39;) { return { statusCode: 409, headers: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, body: JSON.stringify({ error: \u0026#39;Email already exists\u0026#39; }) }; } return { statusCode: 500, headers: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, body: JSON.stringify({ error: \u0026#39;Internal server error\u0026#39; }) }; } }; Step 5: Create Lambda Function - Emergency Requests Create function: daivietblood-emergency-requests Same configuration Add code: const mysql = require(\u0026#39;mysql2/promise\u0026#39;); let connection; const getConnection = async () =\u0026gt; { if (!connection) { connection = await mysql.createConnection({ host: process.env.DB_HOST, port: process.env.DB_PORT, user: process.env.DB_USER, password: process.env.DB_PASSWORD, database: process.env.DB_NAME }); } return connection; }; exports.handler = async (event) =\u0026gt; { const conn = await getConnection(); const method = event.httpMethod; try { if (method === \u0026#39;GET\u0026#39;) { const [rows] = await conn.execute( \u0026#39;SELECT * FROM emergency_requests WHERE status = \u0026#34;open\u0026#34; ORDER BY urgency DESC, created_at DESC\u0026#39; ); return { statusCode: 200, headers: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, body: JSON.stringify(rows) }; } if (method === \u0026#39;POST\u0026#39;) { const body = JSON.parse(event.body); const { requester_name, blood_type, units_needed, hospital, urgency } = body; const [result] = await conn.execute( \u0026#39;INSERT INTO emergency_requests (requester_name, blood_type, units_needed, hospital, urgency) VALUES (?, ?, ?, ?, ?)\u0026#39;, [requester_name, blood_type, units_needed, hospital, urgency || \u0026#39;normal\u0026#39;] ); return { statusCode: 201, headers: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, body: JSON.stringify({ id: result.insertId, message: \u0026#39;Emergency request created\u0026#39; }) }; } return { statusCode: 405, headers: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, body: JSON.stringify({ error: \u0026#39;Method not allowed\u0026#39; }) }; } catch (error) { console.error(\u0026#39;Error:\u0026#39;, error); return { statusCode: 500, headers: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, body: JSON.stringify({ error: \u0026#39;Internal server error\u0026#39; }) }; } }; Verification Checklist IAM Role created with VPC and Basic execution permissions Lambda Layer created with mysql2 package Lambda functions created and deployed: daivietblood-get-users daivietblood-create-user daivietblood-emergency-requests All functions configured with VPC (Private Subnets) Environment variables set correctly Functions deployed successfully "},{"uri":"https://lhoanggg.github.io/intership-report/5-workshop/5.3-s3-vpc/5.3.1-create-gwe/","title":"Create VPC","tags":[],"description":"","content":"Step 1: Create VPC Go to VPC Console → Your VPCs → Create VPC\nConfigure VPC:\nResources to create: VPC and more Name tag auto-generation: daivietblood IPv4 CIDR block: 10.0.0.0/16 IPv6 CIDR block: No IPv6 CIDR block Tenancy: Default Configure Subnets:\nNumber of Availability Zones: 2 Number of public subnets: 2 Number of private subnets: 2 Customize subnets CIDR blocks: Public subnet CIDR block in ap-southeast-1a: 10.0.1.0/24 Public subnet CIDR block in ap-southeast-1b: 10.0.2.0/24 Private subnet CIDR block in ap-southeast-1a: 10.0.3.0/24 Private subnet CIDR block in ap-southeast-1b: 10.0.4.0/24 Configure NAT Gateway:\nNAT gateways: In 1 AZ Configure VPC Endpoints:\nVPC endpoints: None (we\u0026rsquo;ll create later if needed) Click Create VPC\nℹ️ VPC creation takes 2-3 minutes. Wait until status shows \u0026ldquo;Available\u0026rdquo;.\nStep 2: Verify VPC Resources After creation, verify the following resources were created:\nResource Name Details VPC daivietblood-vpc 10.0.0.0/16 Public Subnet 1 daivietblood-subnet-public1-ap-southeast-1a 10.0.1.0/24 Public Subnet 2 daivietblood-subnet-public2-ap-southeast-1b 10.0.2.0/24 Private Subnet 1 daivietblood-subnet-private1-ap-southeast-1a 10.0.3.0/24 Private Subnet 2 daivietblood-subnet-private2-ap-southeast-1b 10.0.4.0/24 Internet Gateway daivietblood-igw Attached to VPC NAT Gateway daivietblood-nat-public1-ap-southeast-1a In Public Subnet 1 Route Table (Public) daivietblood-rtb-public Routes to IGW Route Table (Private) daivietblood-rtb-private1-ap-southeast-1a Routes to NAT Step 3: Create Security Groups 3.1. Security Group for Lambda\nGo to VPC Console → Security Groups → Create security group\nConfigure:\nSecurity group name: daivietblood-lambda-sg Description: Security group for Lambda functions VPC: Select daivietblood-vpc Inbound rules: (Leave empty - Lambda initiates connections)\nOutbound rules:\nType Protocol Port Destination Description All traffic All All 0.0.0.0/0 Allow all outbound Click Create security group\n3.2. Security Group for RDS\nGo to VPC Console → Security Groups → Create security group\nConfigure:\nSecurity group name: daivietblood-rds-sg Description: Security group for RDS MySQL VPC: Select daivietblood-vpc Inbound rules:\nType Protocol Port Source Description MySQL/Aurora TCP 3306 daivietblood-lambda-sg Allow Lambda access Outbound rules:\nType Protocol Port Destination Description All traffic All All 0.0.0.0/0 Allow all outbound Click Create security group\n⚠️ Security Best Practice: Only allow access from Lambda Security Group to RDS. Never open port 3306 to 0.0.0.0/0.\nStep 4: Create DB Subnet Group Go to RDS Console → Subnet groups → Create DB subnet group\nConfigure:\nName: daivietblood-db-subnet-group Description: Subnet group for DaiVietBlood RDS VPC: Select daivietblood-vpc Add subnets:\nAvailability Zones: Select ap-southeast-1a and ap-southeast-1b Subnets: Select both Private Subnets (10.0.3.0/24 and 10.0.4.0/24) Click Create\nVerification Checklist VPC created with CIDR 10.0.0.0/16 2 Public Subnets created 2 Private Subnets created Internet Gateway attached to VPC NAT Gateway created in Public Subnet Route tables configured correctly Lambda Security Group created RDS Security Group created with inbound rule from Lambda SG DB Subnet Group created with Private Subnets "},{"uri":"https://lhoanggg.github.io/intership-report/4-eventparticipated/4.2-event2/","title":"Event 2","tags":[],"description":"","content":"Data Resiliency in a Cloud-first World Event Objectives Share best practices in modern application design. Introduce DDD methods and event-driven architecture. Guide on selecting appropriate compute services. Introduce AI tools supporting the development lifecycle. List of Speakers Paul Haverfield - Principal Storage Specialist BDM, APJ Tamelly Lim - Specialist Solutions Architect Ameen Khan S - GTM specialist for Storage - Data \u0026amp; AI pillar covering ASEAN markets Paul Hidalgo - Key Highlights Today, I had the opportunity to attend an AWS program covering an extremely urgent topic in the current context: Data Resiliency. More than just simple backups, the event opened up new perspectives on protecting digital assets against increasingly sophisticated threats.\nHere are the key takeaways I gathered:\n1. Redefining: How is Data Resiliency different from High Availability (HA) and Disaster Recovery (DR)? Previously, we often focused on HA (ensuring systems are always online) or DR (recovering from physical disasters). However, Data Resiliency is a broader and more \u0026ldquo;proactive\u0026rdquo; concept:\nContext: \u0026ldquo;Everything fails, all the time\u0026rdquo; (Werner Vogels) – Everything is prone to failure, including physical keys or hardware. The Difference: While HA handles infrastructure incidents, Data Resiliency focuses on data integrity. It is the organizational ability to maintain operations, withstand, and recover even when under cyberattack (such as Ransomware) or human error. Goal: To detect anomalies and automate response processes without human intervention. 2. Why has Data Resiliency become an \u0026ldquo;Absolute Necessity\u0026rdquo;? The explosion of data creation comes with new technological vulnerabilities. Three main trends are driving the shift from Protection to Resiliency:\nRegulatory: Compliance with strict data protection laws. Technology: The complexity of Multi-cloud and Hybrid-cloud environments. Threat Landscape: Ransomware no longer just encrypts primary data; it also targets backups. 3. Data Immutability: The Impenetrable Shield A keyword mentioned repeatedly was Data Immutability.\nThis refers to the ability to create data copies that cannot be changed or deleted for a set period. In the event of a Ransomware attack, even if hackers possess top-level admin rights, they cannot alter this backup. It acts as the \u0026ldquo;Last line of defense,\u0026rdquo; ensuring that at least one clean version always exists for recovery. 4. Protection Strategy: The AWS 3-2-1-1-0 Model The traditional 3-2-1 backup rule has been upgraded to suit the cloud era:\n3 copies of data. 2 different storage media. 1 off-site copy (different region). 1 offline or Immutable (Air-gapped) copy. 0 errors during recovery (verified by automated testing). Important concepts to remember:\nRPO (Recovery Point Objective): How much data loss is acceptable? RTO (Recovery Time Objective): How long does it take to get the system running again? Backup Vault: A container for storing backups, encrypted by AWS KMS for enhanced security. 5. Tool Ecosystem \u0026amp; Solutions The event introduced powerful integrated solutions on AWS:\nCommvault Cloud on AWS: Provides Air Gap Protect (secure data isolation). Cloud Rewind: The ability to \u0026ldquo;rewind\u0026rdquo; time to restore entire instances or VPCs as if the incident never happened. Clumio: An all-in-one simplified backup solution. Uses Serverless Workflow architecture (an army of Lambda functions) to optimize costs and operations. Elastio: Focuses on: Detect, Respond, Recover. Scans for Malware/Ransomware directly within Snapshots to ensure backups do not contain latent malicious code. 6. Workshop Architecture: Real-world Implementation During the hands-on session, we deployed a comprehensive protection model:\nKey Components:\nSource: EC2 Instances (EBS) and S3 Buckets containing critical data. Mechanism: Using AWS Backup Plan with an hourly schedule. Protection Layers: Primary: Stored in a standard Vault (workshop-sources-regular-vault). Secondary (Air-gapped): Copied to another Region (us-east-1-LAG-Vault) with Immutability settings enabled. Validation: Integrated Elastio with AWS Backup. Automated Malware scanning on backups. Performed hourly Restore Testing to ensure backup viability (the \u0026ldquo;0 error\u0026rdquo; strategy). Conclusion The event shifted my mindset from merely \u0026ldquo;backing up data\u0026rdquo; to \u0026ldquo;building resiliency.\u0026rdquo; In an era where cyberattacks are inevitable, possessing a Data Resiliency strategy featuring Immutability and Automation (using tools like Elastio or Commvault) is vital for business survival.\nSome photos from the event Add your photos here Overall, the event not only provided technical knowledge but also helped me change my thinking regarding application design, system modernization, and more effective team collaboration.\n"},{"uri":"https://lhoanggg.github.io/intership-report/4-eventparticipated/4.3-event3/","title":"Event 3","tags":[],"description":"","content":"Data Science on AWS Event Objectives Share essential services for data processing (sentiment analysis, comment classification, etc.). List of Speakers Van Hoang Kha - Cloud Solutions Architect, AWS Community Builder Bach Doan Vuong - Cloud DevOps Engineer, AWS Community Builder Key Highlights Below is a summary of the event content focusing on the listed services, presented in a professional report style without icons as requested.\nAWS TRAINING PROGRAM SUMMARY REPORT: AI \u0026amp; MACHINE LEARNING\n1. Overview of Technology Concepts\nTo begin the program, we systematized important foundational concepts in the field of intelligent technology:\nAI (Artificial Intelligence): An overarching concept regarding the creation of intelligent systems. ML (Machine Learning): A subset of AI that allows computers to learn from data. DL (Deep Learning): Uses complex neural networks to model patterns in data. GenAI (Generative AI): Focuses on creating new content and data. 2. AWS as a Service Provider\nThe next section introduced AWS as a comprehensive service provider. AWS offers Managed Services that help businesses apply AI quickly without investing heavily in building infrastructure from scratch.\n3. Details of Introduced AWS Services\nThe training focused deeply on specific tools designed to solve real-world business problems:\nAmazon Comprehend (Natural Language Processing Service - NLP) This service was discussed in the most detail, featuring powerful multi-language text processing capabilities:\nSentiment Analysis: Automatically classifies customer reviews and comments based on positive, negative, or neutral nuances. Text Summarization: Condenses content from long documents. Large-scale Information Processing: Supports bulk email processing and classification. Information Security: Capable of identifying, classifying, and protecting sensitive Personally Identifiable Information (PII) within text. Other Language and Text Processing Services\nAmazon Translate: Automated language translation service. Amazon Textract: A tool for extracting data from scanned documents and papers, including handwriting and complex forms. Amazon Transcribe: A service for converting speech (audio) into written text. Image and Computer Vision Services\nAmazon Rekognition: A Deep Learning-based service specialized for analyzing images and videos (object detection, facial recognition, content moderation). Customer Experience Services\nAmazon Personalize: A solution to enhance customer experience through personalization. This service records and analyzes user behavior, thereby providing product or content recommendations best suited to individual preferences. Technical Infrastructure\nSageMaker Instance: Provides the server environment and tools necessary for developers to self-build, train, and deploy custom machine learning models according to specific needs. Some photos from the event Add your photos here Overall, the event not only provided technical knowledge but also helped me change my thinking regarding application design, system modernization, and more effective team collaboration.\n"},{"uri":"https://lhoanggg.github.io/intership-report/4-eventparticipated/4.4-event4/","title":"Event 4","tags":[],"description":"","content":"Summary Report: Attending AWS Cloud Mastery Series #1 - AI/ML/GenAI on AWS\nAttendance Objectives Recently, I had the opportunity to attend the opening event for the \u0026ldquo;AWS Cloud Mastery\u0026rdquo; series, focusing on AI, Machine Learning, and Generative AI. My main goal was to update my comprehensive view of these technologies on the AWS platform and learn how to apply them to real-world business problems.\nSpeakers The session featured sharing from experienced experts in the industry, including Mr. Lam Tuan Kiet (Sr DevOps Engineer - FPT Software), Mr. Danh Hoang Hieu Nghi (AI Engineer - Renova Cloud), Mr. Dinh Le Hoang Anh (Cloud Engineer Trainee), and Mr. Van Hoang Kha (Community Builder).\nValuable Knowledge I Harvested:\n1. The Power of Generative AI on Amazon Bedrock This was the part that impressed me the most. Amazon Bedrock acts as a central platform, providing access to leading Foundation Models (FMs) from Anthropic, OpenAI, Meta, etc. This allows us to fine-tune existing models without having to build a model from scratch.\nI also reinforced my skills in Prompt Engineering, understanding better how to guide the model through various strategies:\nZero-shot: Providing a request directly without examples. Few-shot: Providing a handful of examples for the model to mimic. Chain-of-Thought: Asking the model to explain its reasoning steps for a more logical result. Specifically, the RAG (Retrieval Augmented Generation) technique was highlighted as an optimal solution to improve accuracy:\nRetrieval: Pulling real data from the enterprise knowledge base. Augmentation: Adding that data as context for the prompt. Generation: The model answers based on factual information, minimizing hallucinations. Additionally, Amazon Titan Embeddings was introduced as a tool to convert text into vectors, serving semantic search and multilingual RAG workflows.\n2. AWS AI Services Ecosystem Beyond GenAI, I also reviewed AWS\u0026rsquo;s \u0026ldquo;ready-made\u0026rdquo; AI services (APIs) that help solve specific problems quickly without complex model training:\nImage/Video Analysis (Rekognition). Translation (Translate) and Speech-to-Text/Text-to-Speech (Transcribe, Polly). Data Extraction (Textract) and Natural Language Processing (Comprehend). Intelligent Search (Kendra) or Anomaly Detection (Lookout). The AMZPhoto face recognition demo visually illustrated how to integrate these services into a real product.\n3. Amazon Bedrock AgentCore – Putting AI Agents into Practice This is a new framework helping to solve the problem of operating AI Agents at scale (production-ready). It supports long-term memory management, identity security, tool integration (like browsers, code interpreters), and most importantly, observability. This makes deploying frameworks like CrewAI or LangGraph safer and more effective on the AWS platform.\nPlan for Application at Work Based on what I learned, I plan to apply the following knowledge immediately:\nDeploy RAG \u0026amp; AgentCore: Propose and apply these to upcoming internal projects requiring GenAI features to increase accuracy and automation capabilities. Optimize Development Process: Use available AWS AI Services instead of building from scratch to shorten Time-to-market. Improve Model Performance: Apply advanced Prompt Engineering techniques to optimize output results for current AI tasks. Side Experience Not only did I absorb knowledge, but the event atmosphere was also extremely lively. I was lucky enough to reach the Top 6 in the Kahoot competition; however, I dropped off the leaderboard during the final questions. Nevertheless, it was still an interesting game that helped me reinforce the knowledge from this event.\n"},{"uri":"https://lhoanggg.github.io/intership-report/4-eventparticipated/4.5-event5/","title":"Event 5","tags":[],"description":"","content":"Summary Report: Attending AWS Cloud Mastery Series #2 – DevOps on AWS Attendance Objectives Continuing the event series, I attended the second session focusing on DevOps. My goal was to master AWS services supporting DevOps, deepen my understanding of CI/CD pipeline design, Infrastructure as Code (IaC) concepts, as well as how to deploy and monitor containerized applications on the AWS platform.\nSpeakers The session gathered a strong lineup of speakers, including AWS Community Builders and experienced engineers:\nMr. Truong Quang Tinh – Platform Engineer (TymeX) Mr. Bao Huynh, Nguyen Khanh Phuc Thinh, Tran Dai Vi, Huynh Hoang Long, Pham Hoang Quy, Nghiem Le (AWS Community Builders) Mr. Dinh Le Hoang Anh – Cloud Engineer Trainee (First Cloud AI Journey) Valuable Knowledge I Harvested: 1. Building a DevOps Foundation The speakers helped me redefine that DevOps is not a job title but a mindset and habit of working. The core lies in:\nAutomating repetitive tasks. Sharing knowledge across teams. Continuously experimenting and learning. Measuring effectiveness with real data rather than assumptions. I also learned lessons about common pitfalls for beginners: avoid getting stuck in \u0026ldquo;tutorial hell\u0026rdquo; without starting real projects, and focus on personal progress rather than comparing oneself to others.\n2. Infrastructure as Code (IaC) in Practice This section broadened my horizon regarding IaC tools instead of sticking to a single one. The speakers provided a detailed comparison:\nCloudFormation: AWS\u0026rsquo;s native template tool. AWS CDK: For developers who prefer writing infrastructure using familiar programming languages. Terraform: The optimal choice for teams working on multi-cloud platforms. The most important message: Infrastructure built via Code (IaC) is significantly more consistent, maintainable, and secure compared to manual configuration (ClickOps).\n3. Containers and Deployment Models The content ranged from basics (Dockerfile, Image) to advanced AWS services:\nAmazon ECR: Storage and security scanning for images. Amazon ECS \u0026amp; EKS: Two popular container orchestration options. The comparison between ECS (simple, native) and EKS (powerful with Kubernetes) helped me know when to use which. AWS App Runner: A quick deployment solution without worrying about cluster management. 4. Monitoring and Observability A system cannot lack monitoring capabilities. I understood better the roles of:\nAmazon CloudWatch: The center for metrics, logs, and alarms. AWS X-Ray: A tracing tool helping visualize request flows and detect bottlenecks. Core lesson: Observability features must be designed from the very beginning, not added after the system is built.\nPlan for Application at Work Specifically, I plan to apply this knowledge to the team\u0026rsquo;s upcoming AI Chatbot Project:\nEstablish CI/CD Pipeline: Use AWS CodePipeline to automate the entire process from Build, Test, to Deploy. The goal is to ensure every code update is tested and pushed to production smoothly. Implement IaC: Use AWS CDK to define all resources (Lambda, API Gateway, DynamoDB, S3, IAM\u0026hellip;). This makes the system easy to reuse for different environments and scale quickly when needed. Applying this process will help the Chatbot project develop faster, minimize human error, and make operations easier.\nEvent Experience The session gave me a practical perspective on how modern businesses implement DevOps on AWS. Beyond theory, the real-world examples from speakers about CloudFormation, Terraform, or how to choose EKS/ECS were truly valuable.\nBeyond professional knowledge, this was also a good occasion for me to network with like-minded friends and learn \u0026ldquo;hard-earned\u0026rdquo; experiences from those who came before me.\n"},{"uri":"https://lhoanggg.github.io/intership-report/5-workshop/5.1-workshop-overview/","title":"Workshop Overview","tags":[],"description":"","content":"System Architecture The DaiVietBlood system uses a Serverless-First architecture on AWS Cloud, prioritizing scalability, security, and operational optimization.\nArchitecture Components 1. Network Infrastructure (VPC)\nComponent Description VPC Virtual Private Cloud with CIDR 10.0.0.0/16 Public Subnet Contains NAT Gateway, allows Internet access Private Subnet Contains Lambda, RDS - isolated from Internet NAT Gateway Allows Private Subnet resources to access Internet Internet Gateway Allows Public Subnet to communicate with Internet 2. Application \u0026amp; Data Layer\nService Role AWS Lambda Process business logic (CRUD operations, emergency requests) API Gateway Receive HTTP requests, route to Lambda Amazon RDS MySQL database storing user data, blood inventory Amazon S3 Store static files (images, documents) 3. Frontend \u0026amp; Distribution\nService Role AWS Amplify Host React application CloudFront CDN distributes content globally with low latency 4. DevOps \u0026amp; Monitoring\nService Role CodePipeline Automate CI/CD process CodeBuild Build and test source code CloudWatch Collect logs, metrics, set up alarms Data Flow User Request → CloudFront → Amplify (Frontend) ↓ API Gateway ↓ AWS Lambda (Private Subnet) ↓ Amazon RDS (Private Subnet) Security Model Network Isolation: RDS and Lambda in Private Subnet, no direct Internet access IAM Roles: Each service has minimum required permissions (Least Privilege) Data Encryption: At-rest (RDS, S3) and In-transit (HTTPS) Security Groups: Control inbound/outbound traffic for each resource Workshop Objectives After completing this workshop, you will be able to:\n✅ Create VPC with proper network segmentation ✅ Deploy RDS MySQL in Private Subnet ✅ Build Lambda functions and expose via API Gateway ✅ Configure S3 and CloudFront for static content ✅ Deploy React app with Amplify ✅ Set up CI/CD pipeline ✅ Monitor with CloudWatch "},{"uri":"https://lhoanggg.github.io/intership-report/3-blogstranslated/3.1-blog1/","title":"Blog 1","tags":[],"description":"","content":"Monitoring Server Health with Amazon GameLift Servers Author: Brian Schuster | Date: 20 NOV 2025 | Category: Amazon GameLift, Amazon Managed Grafana, Game Development, Games, Management Tools\nRunning a successful multiplayer game means constantly balancing performance, scalability, and player experience. As the player base grows, new challenges emerge. The causes could stem from various issues: perhaps something in your server code. There could be a memory leak, inefficient logic, or a bug that only surfaces under specific conditions.\nAdditionally, optimizing latency using Amazon GameLift Servers is important, by strategically allocating capacity across regions to ensure players connect to servers closest to them. However, even with latency optimizations in place, you may still receive feedback about poor performance or degraded gameplay from a subset of players.\nWe have previously discussed how to diagnose and address broader network issues, but latency metrics alone won’t provide the full picture. You need deeper visibility into what is happening on your game servers.\nServer-side observability: beyond latency When using Amazon GameLift Servers with telemetry enabled, your game servers collect a variety of detailed metrics — including resource utilization (CPU, memory, network), game-specific metrics (such as tick rate), and custom metrics you define. These metrics are captured at different levels so you can analyze performance at the process, container, host, or fleet-wide level. They can be sent to your chosen metrics storage and visualized with your preferred tools.\nWe will demonstrate using Amazon Managed Grafana with pre-built dashboards. The setup is streamlined and can be completed in under an hour.\nThe real power lies not only in the metrics themselves but also in how the dashboards help you quickly identify and diagnose issues.\nTroubleshooting game server crashes A common scenario: game sessions are crashing, players are disconnected mid-match, and you need to determine why.\nIf you haven’t already set up monitoring dashboards, follow the Configure Amazon Grafana guide — it takes a few minutes to provision pre-built dashboards for your fleet. Note that we are following the C++ SDK steps, adjust for your chosen implementation.\nOnce configured, open Amazon Managed Grafana from the AWS Management Console and navigate to the EC2 Fleet Overview dashboard. Figure 1 shows a crash event in the Game Server Crashes graph, while the Crashed Game Sessions section displays details of the specific crashed game session. Both the instance and the crashed session are linked for deeper investigation.\nFigure 1: EC2 Fleet Overview dashboard showing a crashed game session.\nSelect the affected instance. The memory graph (Figure 2) shows memory spiked sharply, then dropped as the process crashed — a signature of a memory leak. Looking at session-level details, you can see one session used significantly more memory.\nFigure 2: Instance Performance dashboard showing memory leak.\nClicking on the crashed game session opens the Server Performance dashboard (Figure 3), showing the session’s resource usage up to the moment of crash. The dashboard indicates this session was responsible for the memory leak.\nFigure 3: Server Performance dashboard showing memory leak.\nEach graph includes tooltips explaining what to look for and how to interpret the data. The next step is clear: investigate the logs of the crashed session to identify the trigger, whether from a specific game mode or a player’s action. Metrics guide you to the right logs.\nTroubleshooting high CPU usage Another scenario: players report stuttering gameplay but no crashes. The issue might be CPU-related.\nSwitch to the EC2 Instances Overview dashboard in Amazon Managed Grafana. Figure 4 shows the top 20 EC2 instances by CPU consumption. Most hover around 2–3% CPU, but a few reach 20–30% or higher.\nFigure 4: EC2 Instances Overview dashboard\nSelect a high-CPU instance. The dashboard breaks down CPU by game session (Figure 5), immediately showing which session is consuming the most resources. You can then review logs for that session, focusing on periods of elevated CPU usage.\nFigure 5: Instance Performance dashboard showing top CPU-consuming game sessions.\nYou may find high CPU correlates with intense combat scenarios or a pathfinding bug causing excessive calculations. Metrics alone don’t tell you exactly what’s wrong, but they indicate where to look.\nContainer support If running your GameLift Servers Fleet with containers instead of EC2, the same troubleshooting approach applies. Figure 6 is the Container Fleet Overview dashboard, showing tasks with high CPU or memory usage.\nFigure 6: Container Fleet Overview dashboard\nClick a specific task, and the Container Performance dashboard (Figure 7) breaks down metrics by individual containers. You can see if the game server container is consuming resources as expected or if a sidecar container is causing issues. This granularity helps quickly isolate problems, whether running on EC2 or containers.\nFigure 7: Container Performance dashboard\nNext steps Beyond built-in hardware and game metrics (tick rate, crashes), you can extend monitoring with custom metrics.\nExamples of game-specific metrics:\nCombat balance: Average time-to-kill or DPS by weapon type to detect overpowered weapons after a patch. Progression blockers: Success rate of critical quests or bosses to detect bugs blocking progression. Economy health: Currency inflation or item acquisition patterns to spot exploits. AI pathfinding duration: Time spent calculating NPC paths to detect complex scenarios affecting performance. After instrumenting custom metrics, create alerts in Amazon Managed Grafana for both system and game-specific thresholds. Examples:\nMemory \u0026gt; 90% CPU \u0026gt; 85% sustained Increase in crashed sessions Spikes in custom metrics (e.g., failed boss attempts) When an alert triggers, click into the relevant dashboard to investigate — catching issues before players notice.\nConclusion Optimizing latency connects players to the right location, but server-side observability ensures a smooth in-game experience. GameLift Servers telemetry with pre-built dashboards provides visibility to quickly diagnose crashes, performance bottlenecks, and resource issues — without being overwhelmed by raw metrics.\nNext time a player complains about a glitchy experience, you’ll know exactly where to look, and with proactive alerting, you’ll catch the issue before they notice.\nContact an AWS Representative to learn how we can help accelerate your business.\nFurther reading:\nGetting started with Amazon GameLift Servers Setting up alerts in Amazon Managed Grafana Available Amazon GameLift Servers dashboards and metrics Author: Brian Schuster, Principal Engineer at AWS for Amazon GameLift.\n"},{"uri":"https://lhoanggg.github.io/intership-report/3-blogstranslated/3.2-blog2/","title":"Blog 2","tags":[],"description":"","content":"Advanced analytics using Amazon CloudWatch Logs Insights Authors: Joe Alioto \u0026amp; Dot Ho | Date: 27 AUG 2025 | Category: Amazon CloudWatch, Observability, Logs, Analytics\nEffective log management and analysis are critical for maintaining system stability, security, and high performance. Amazon CloudWatch Logs Insights has long been a powerful tool for searching, filtering, and analyzing log data from multiple log groups. The addition of OpenSearch Piped Processing Language (PPL) and OpenSearch SQL support provides greater flexibility and familiarity for log analytics.\nWhether you are a developer debugging application issues, a security analyst investigating threats, or an operations manager monitoring system performance — these new capabilities empower you to:\nUse AI-powered natural language query generation and automatic result summarization — reducing the time from raw logs to insights. Easily correlate logs using JOINs across multiple log groups. Leverage a rich function library for processing and analyzing log data (JSON parsing, mathematical functions, string manipulation\u0026hellip;) without needing external tools. Correlate data from various log sources — ideal for complex systems, microservices, and distributed architectures. Key capabilities of PPL \u0026amp; SQL in CloudWatch Logs Insights Familiar SQL-like query language You can use SQL to analyze logs instead of learning a separate domain-specific language. Example:\nSELECT eventName, COUNT(*) as count FROM loggroupname GROUP BY eventName ORDER BY count DESC LIMIT 10 Correlation capabilities — JOIN across multiple log groups You can join data from different log groups to obtain a more complete picture. Example:\nSELECT a.transaction_id, a.error_message AS application_error, a.timestamp AS application_timestamp, i.error_message AS infrastructure_error, i.timestamp AS infrastructure_timestamp FROM application_logs a LEFT JOIN infrastructure_logs i ON a.transaction_id = i.transaction_id This is especially useful in microservices environments, where a transaction often traverses multiple services.\nFunction library for processing \u0026amp; analyzing complex log data CloudWatch Logs Insights supports JSON functions, mathematical functions, string functions, and more. For example:\nSELECT JSON_EXTRACT_SCALAR(message, \u0026#39;$.user_id\u0026#39;) as user_id, AVG(CAST(JSON_EXTRACT_SCALAR(message, \u0026#39;$.response_time\u0026#39;) AS DOUBLE)) as avg_response_time FROM loggroupname GROUP BY JSON_EXTRACT_SCALAR(message, \u0026#39;$.user_id\u0026#39;) This allows deeper analysis without exporting logs elsewhere.\nAdvanced queries: subqueries \u0026amp; advanced aggregations You can write subqueries or perform advanced aggregations to analyze uncommon patterns or behaviors:\nSELECT user_id, api_calls FROM ( SELECT user_id, COUNT(*) as api_calls FROM api_logs GROUP BY user_id ) subquery WHERE api_calls \u0026gt; (SELECT AVG(api_calls) * 2 FROM subquery) Fast query generation \u0026amp; analysis using AI + on-demand anomaly detection Natural language query generation \u0026amp; summarization You can type a simple English command such as:\n\u0026ldquo;Get api count by eventSource and eventName and sort it by most to least\u0026rdquo;\n→ CloudWatch Logs Insights automatically generates a valid query (PPL / SQL / Logs Insights QL). After execution, the system can summarize the results in natural language — helping you quickly understand insights without scanning hundreds or thousands of log lines.\nOn-demand anomaly detection A notable feature: you can detect unusual patterns instantly — without prior setup:\nUse pattern @message | anomaly to identify anomalies. Returned fields include: @description, @anomalyLogSamples, @priority, @priorityScore, @patternString. You can also use compare to compare patterns across time ranges — useful for investigating differences between current and historical logs. Typical use cases:\nSecurity monitoring: detect unusual access, login failures\u0026hellip; Application health monitoring: detect error spikes, latency issues\u0026hellip; Infrastructure monitoring: identify abnormal resource usage\u0026hellip; Correlation — subqueries \u0026amp; JOINs across multiple log groups With JOINs and subqueries, you can correlate data across log groups — enabling end-to-end transaction tracing, cross-service debugging, and multi-service event investigation.\nExample: find all orders whose payment status is \u0026ldquo;completed\u0026rdquo;.\nSQL:\nSELECT correlationId, timestamp FROM `/orders/prod` o WHERE EXISTS ( SELECT 1 FROM `/payments/prod` p WHERE p.correlationId = o.correlationId AND p.paymentStatus = \u0026#39;completed\u0026#39; ) ORDER BY timestamp DESC LIMIT 10; PPL:\nfields timestamp, correlationId | where `correlationId` IN [ search source=`/payments/prod` | where paymentStatus=\u0026#34;completed\u0026#34; | fields `correlationId` ] | head 10 You can also use JOINs to combine full payment + order data (orderId, paymentStatus, amount, payment method…).\nWhen working with correlation, pay attention to appropriate time ranges, indexing common fields, performance on large datasets, and handling missing or null correlationId values.\nConclusion The enhancements to CloudWatch Logs Insights through PPL \u0026amp; SQL support significantly expand its log management and analytics capabilities:\nMore accessible using familiar query languages (SQL). Deeper analytics through function libraries \u0026amp; advanced aggregations. Cross-log-source correlation enables stronger cross-service debugging \u0026amp; analysis. Generative AI shortens the time from raw logs → actionable insights. On-demand anomaly detection enables fast and early issue detection. Logs Insights is no longer just a log search tool — it is a comprehensive log analytics platform, ideal for distributed systems, microservices architectures, and environments requiring high observability.\nFor production use, consider defining indexes on frequently queried fields to improve performance, and regularly review saved queries as your system evolves.\n"},{"uri":"https://lhoanggg.github.io/intership-report/3-blogstranslated/3.3-blog3/","title":"Blog 3","tags":[],"description":"","content":"Handling sensitive log data using Amazon CloudWatch Authors: Amazon Web Services (AWS) | Date: 2025 (?) | Category: Amazon CloudWatch, Security, Logs \u0026amp; Observability\nEfficient logging is essential to building effective investigative and response workflows. Logs, metrics, and traces provide critical value when debugging applications, handling security events, or resolving failures. However, wide-event structured logs significantly raise the likelihood of exposing sensitive information — particularly personally identifiable information (PII) — stored in log data. :contentReference[oaicite:16]{index=16}\nThis article outlines common techniques to secure sensitive information in logs — such as data masking and access control — while maintaining a low mean time to respond (MTTR) when debugging or handling incidents. :contentReference[oaicite:17]{index=17}\nWhat is Personally Identifiable Information (PII)? As defined by the NIST Computer Security Resource Center, PII includes:\nAny information that can be used to distinguish or trace an individual’s identity — e.g. name, social security number, date of birth, biometric records; Any information linked or linkable to a person — e.g. medical data, financial records, employment information. :contentReference[oaicite:18]{index=18} Modern applications often collect PII to personalize user experiences or support business workflows — e.g. payment info, personal profile, etc. Structured logging plus telemetry and correlation improve debugging and issue investigation speed — but also increase risk of sensitive data exposure if logs aren’t protected. :contentReference[oaicite:19]{index=19}\nStrategies to Protect Sensitive Log Data with CloudWatch Data masking \u0026amp; access control using CloudWatch + IAM You can enable CloudWatch Logs’ data protection / data masking feature — sensitive data will be masked (redacted / obfuscated) when logged or displayed. :contentReference[oaicite:20]{index=20} Only users with special privileges (logs:Unmask) can view unmasked data — enabling role-based access control and reducing unauthorized exposure risk. :contentReference[oaicite:21]{index=21} The policy can be applied at the account level (all log groups) or per-log-group level, depending on needs and compliance requirements. :contentReference[oaicite:22]{index=22} Support for various sensitive data types (PII, PHI, financial, credentials, device identifiers, etc.) CloudWatch Logs supports “managed data identifiers” to detect and protect many categories of sensitive data:\nPersonal identifiable information (PII): name, address, email, IP address, etc. :contentReference[oaicite:23]{index=23} Financial information: credit card numbers, payment data. :contentReference[oaicite:24]{index=24} Other sensitive categories: credentials (private keys, secret access keys), device identifiers, protected health information (PHI), etc. :contentReference[oaicite:25]{index=25} Audit \u0026amp; Findings + Alerts for sensitive data detection When a data protection policy is active, any log event containing sensitive data generates a “finding.” You can log these findings, send audit reports to CloudWatch Logs, Amazon S3, or Amazon Data Firehose for compliance and monitoring. :contentReference[oaicite:26]{index=26} You can also create alarms based on the vended metric LogEventsWithFindings — to be notified automatically whenever sensitive data appears in logs. :contentReference[oaicite:27]{index=27} When to Enable Masking \u0026amp; Data Protection Your application handles sensitive data: PII, payments, authentication, health data, etc. You need to comply with data protection regulations (e.g. GDPR, PCI-DSS, HIPAA) depending on your region and industry. :contentReference[oaicite:28]{index=28} You want to balance between quick debugging / incident response and secure handling of sensitive data — CloudWatch offers built-in support to mask data while preserving log functionality. Conclusion Comprehensive logging is vital for observability, debugging, and incident response. But when logs contain sensitive information — you must protect them properly.\nCloudWatch Logs’ data protection and masking features help you:\nReduce the risk of sensitive data leakage (PII, financial info, credentials…) Maintain log analysis capabilities (search, filter, alert, debug) Enforce role-based access via IAM Meet compliance and audit requirements With these features, CloudWatch Logs evolves from a simple logging/monitoring tool to a secure log analytics platform, suitable for production workloads requiring high observability and data privacy.\n"},{"uri":"https://lhoanggg.github.io/intership-report/1-worklog/1.4-week4/","title":"Week 4 Worklog","tags":[],"description":"","content":"Week 4 Objectives: Understand how to deploy applications using Lightsail and Lightsail Containers. Understand how to scale applications automatically using Auto Scaling. Be able to use AWS CLI. Tasks to be implemented this week: Day Task Start Date End Date Reference Material 2 - Learn about Amazon Lightsail and Amazon Lightsail Containers 29/09/2025 29/09/2025 https://000045.awsstudygroup.com/vi/ 3 - Learn how to scale applications using EC2 Auto Scaling - Practice: + Set up Launch Templates + Create EC2 AMIs + Configure Load Balancer + Target Group, Auto Scaling Group + \u0026hellip; 30/09/2025 30/09/2025 https://000006.awsstudygroup.com/vi/ 4 - Learn about AWS CloudWatch 01/10/2025 01/10/2025 https://000008.awsstudygroup.com/vi/ 5 - Learn about AWS Route 53 - Practice: + Create key pair + CloudFormation + Configure Security Group + Set up DNS Outbound Endpoint, Inbound Endpoint + Create Route 53 Rules 02/10/2025 02/10/2025 https://000010.awsstudygroup.com/vi/ 6 - Work again with AWS CLI - Practice: + Install AWS CLI + Interact with AWS services using CLI 03/10/2025 03/10/2025 https://000011.awsstudygroup.com/vi/ Week 4 Achievements: Gained foundational knowledge about deploying applications using Amazon Lightsail \u0026amp; Lightsail Containers.\nUnderstood how Lightsail operates. Learned how to deploy containerized applications in a simple, low-cost environment. Understood the workflow of scaling applications using EC2 Auto Scaling.\nMonitored applications using AWS CloudWatch.\nLearned to use AWS Route 53.\nUsed AWS CLI to interact with AWS services.\n"},{"uri":"https://lhoanggg.github.io/intership-report/1-worklog/","title":"Worklog","tags":[],"description":"","content":"On this page, you will need to introduce your worklog. How did you complete it? How many weeks did you take to complete the program? What did you do in those weeks?\nTypically, and as a standard, a worklog is carried out over about 3 months (throughout the internship period) with weekly contents as follows:\nWeek 1: Onboarding and AWS Fundamentals\nWeek 2: AWS IAM, VPC Architecture \u0026amp; EC2 Deployment\nWeek 3: Deploying and Managing AWS Services with CLI\nWeek 4: AWS Deployment – Scaling – Monitoring – DNS Management\nWeek 5: Strengthening Full-Stack Skills and Exploring NoSQL Databases\nWeek 6: Practicing AWS CLI, S3, CloudFront, and Lambda\nWeek 7: Advanced AWS Practice and Lab Knowledge Consolidation\nWeek 8: Reviewing Learned Knowledge and Midterm Preparation\nWeek 9: Final Project Preparation and Back-End/React Review\nWeek 10: Backend Implementation, Dataset Creation, and DynamoDB Setup\nWeek 11: Writing APIs and Updating DynamoDB for Final Project\nWeek 12: Writing APIs and Updating DynamoDB for Final Project\n"},{"uri":"https://lhoanggg.github.io/intership-report/1-worklog/1.1-week1/","title":"Worklog Week 1","tags":[],"description":"","content":"Week 1 Objectives: Adapt to the working environment at First Cloud Journey. Set up a complete personal AWS environment to prepare for advanced hands-on exercises in the following weeks. Tasks to be completed this week: Day Tasks Start Date Completion Date Reference Materials 2 - Get acquainted with FCJ members - Get familiar with the office working environment - Memorize office rules and regulations 11/08/2025 11/08/2025 3 - Learn about AWS and service categories: + Compute + Storage + Networking + Database + Security, Identity \u0026amp; Compliance + \u0026hellip; 12/08/2025 12/08/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create a new AWS Free Tier account - Set up MFA for AWS account - Explore the AWS Management Console - Practice: + Create Admin Group and Admin User + Verify the account 13/08/2025 13/08/2025 https://000001.awsstudygroup.com/vi/ 5 - Manage cost with AWS Budget - Explore different types of budgets - Practice: + Create budgets (Cost Budget, Usage Budget, RI Budget, Saving Plans Budget) 14/08/2025 15/08/2025 https://000007.awsstudygroup.com/vi/ 6 - Learn how to request support from AWS Support 15/08/2025 15/08/2025 https://000009.awsstudygroup.com/vi/ Week 1 Achievements: Gained a general understanding of AWS:\nStudied AWS service categories: Compute, Storage, Database, Networking, Security, Identity \u0026amp; Compliance, etc. Set up and secured the AWS account.\nUnderstood how IAM works — a core foundation for secure AWS management.\nLearned how to control costs — an essential skill when working with AWS Free Tier.\nLearned how to handle issues and request official support from AWS.\n"},{"uri":"https://lhoanggg.github.io/intership-report/1-worklog/1.10-week10/","title":"Worklog Week 10","tags":[],"description":"","content":"Goals for Week 10: Finalize the service selection and optimize costs for the final project. Review back-end code and enhance front-end skills with React. Review and practice JWT, Authentication, and Authorization. Create and manage datasets on DynamoDB and configure AWS CLI to interact with the database. Tasks to be carried out this week: Day Task Start Date End Date Reference Material 2 - Finalize the services to use with teammates - Continue reviewing services and optimizing costs 10/11/2025 10/11/2025 3 - Review code and learn React 11/11/2025 11/11/2025 4 - Review JWT, Authentication, Authorization - Create a sample database to test APIs 12/11/2025 12/11/2025 5 - Create dataset 13/11/2025 13/11/2025 6 - Upload dataset to DynamoDB - Set up AWS CLI, aws configure to interact with DynamoDB 14/11/2025 14/11/2025 Achievements for Week 10: Finalized service selection and cost optimization for the final project.\nReviewed and consolidated back-end coding skills and enhanced React skills.\nUnderstood and practiced JWT, Authentication, and Authorization.\nCreated and managed datasets on DynamoDB and successfully interacted with the database via AWS CLI.\n"},{"uri":"https://lhoanggg.github.io/intership-report/1-worklog/1.11-week11/","title":"Worklog Week 11","tags":[],"description":"","content":"Goals for Week 11: Attend AWS events to enhance knowledge and stay updated with new trends. Write and deploy APIs for the final project. Configure and manage DynamoDB, update datasets. Tasks to be carried out this week: Day Task Start Date End Date Reference Material 2 - Attend AWS Cloud Mastery Series #2 17/11/2025 17/11/2025 3 - Write APIs for the project 18/11/2025 18/11/2025 4 - Write APIs for the final project 19/11/2025 19/11/2025 5 - Reconfigure DynamoDB and update datasets 20/11/2025 20/11/2025 6 - Write APIs for the final project 21/11/2025 21/11/2025 Achievements for Week 11: Attended AWS event to enhance understanding of services and new trends.\nCompleted writing and deploying APIs for the final project.\nReconfigured DynamoDB and effectively managed and updated datasets.\n"},{"uri":"https://lhoanggg.github.io/intership-report/1-worklog/1.12-week12/","title":"Worklog Week 12","tags":[],"description":"","content":"Goals for Week 12: Complete and update APIs for the final project. Implement and carry out the final project. Tasks to be carried out this week: Day Task Start Date End Date Reference Material 2 - Update APIs for the project 24/11/2025 24/11/2025 3 - Write APIs for the final project 25/11/2025 25/11/2025 4 - Write APIs for the final project 26/11/2025 26/11/2025 5 - Work on the final project 27/11/2025 27/11/2025 6 - Work on the final project 28/11/2025 28/11/2025 Achievements for Week 12: Completed writing and updating APIs for the final project.\nCarried out the final project, applying all the knowledge learned.\n"},{"uri":"https://lhoanggg.github.io/intership-report/1-worklog/1.2-week2/","title":"Worklog Week 2","tags":[],"description":"","content":"Week 2 Objectives: Understand and manage access control with IAM at a practical level. Design and deploy VPC following AWS architectural best practices. Apply network security within the VPC environment. Tasks to be completed this week: Day Tasks Start Date Completion Date Reference Materials 2 - Learn about access control with AWS IAM - Study IAM components such as IAM Group and IAM User 15/09/2025 15/09/2025 https://000002.awsstudygroup.com/vi/ 3 - Practice: + Create IAM Group, IAM User, IAM Role + Perform Role switching + Use Switch Role History 16/09/2025 16/09/2025 https://000002.awsstudygroup.com/vi/1-introduction/ 4 - Learn the fundamentals of VPC - Learn about Network Security with Security Groups and Network ACLs - Design and deploy a VPC following the AWS Well-Architected Framework 17/09/2025 17/09/2025 https://000003.awsstudygroup.com/vi/ 5 - Practice: + Create VPC, Subnet, Internet Gateway, \u0026hellip; + Enable VPC Flow Logs - Learn about EC2 and deploy EC2 Instances 18/09/2025 18/09/2025 https://000003.awsstudygroup.com/vi/4-createec2server/ 6 - Successfully launch EC2 Instances - Deploy and run a Node.js application on Amazon Linux 19/09/2025 19/09/2025 https://000004.awsstudygroup.com/vi/ Week 2 Achievements: Mastered access control and security with AWS IAM.\nUnderstood core IAM components: User, Group, Role. Understood and implemented networking with Amazon VPC.\nGained fundamental knowledge of VPC and AWS network architecture. Understood Network Security. Designed and deployed a VPC following the AWS Well-Architected Framework. Became familiar with EC2.\nLearned details about EC2 Instances. Successfully launched multiple EC2 Instances. Deployed and ran a Node.js application on Amazon Linux. "},{"uri":"https://lhoanggg.github.io/intership-report/1-worklog/1.3-week3/","title":"Worklog Week 3","tags":[],"description":"","content":"Week 3 Objectives: Manage IAM: create Users, Roles, and Access Keys. Practice using AWS CLI. Manage and optimize data on S3 and CloudFront. Tasks to be implemented this week: Day Task Start Date End Date Reference Material 2 - Grant application access to AWS services via IAM Role - Practice: + Create IAM User and Access Key + Create and use IAM Role 22/09/2025 22/09/2025 https://000048.awsstudygroup.com/vi/1-prepare/ 3 - Learn about AWS Cloud9 - First experience with AWS CLI 23/09/2025 23/09/2025 https://000049.awsstudygroup.com/vi/ 4 - Practice: + Run AWS CLI commands from the workshop + Work with text files (.txt) - Learn about S3 24/09/2025 24/09/2025 https://000049.awsstudygroup.com/vi/3-useawscli/ 5 - Learn about AWS Amplify Hosting - Further study of AWS S3 - Practice: + Block Public Access, Public Object + Accelerate static website with CloudFront + Move objects to another region 25/09/2025 25/09/2025 https://000057.awsstudygroup.com/vi/ 6 - Practice AWS S3 again - Learn about Amazon RDS - Practice: + EC2 Instances + RDS Database Instances 26/09/2025 26/09/2025 https://000005.awsstudygroup.com/vi/ Week 3 Achievements: Access management and IAM\nUnderstood how to grant application access to AWS services via IAM Role. Practiced creating IAM Users, Access Keys, and using IAM Roles. Familiarity with AWS CLI and Cloud9\nData management with S3\nLearned about AWS S3 and AWS Amplify Hosting. Practiced configuring public access, accelerating static websites with CloudFront, and moving objects across regions. Working with RDS databases\nCreated EC2 Instances and RDS Database Instances. "},{"uri":"https://lhoanggg.github.io/intership-report/1-worklog/1.5-week5/","title":"Worklog Week 5","tags":[],"description":"","content":"Week 5 Objectives: Use tools to manage work and store personal data. Review and apply AWS Well-Architected Framework in architecture design. Strengthen back-end skills and begin learning front-end development. Understand the basics of NoSQL and DynamoDB. Tasks to be implemented this week: Day Task Start Date End Date Reference Material 2 - Learn how to use Notion to manage schedules, store databases, etc. 06/10/2025 06/10/2025 3 - Review AWS Well-Architected Framework to support the team in drawing architecture 07/10/2025 07/10/2025 4 - Review back-end coding skills 08/10/2025 08/10/2025 5 - Review back-end - Learn more and study React 09/10/2025 09/10/2025 6 - Learn about NoSQL databases - Learn about DynamoDB 10/10/2025 10/10/2025 https://000060.awsstudygroup.com/vi/ Week 5 Achievements: Reviewed AWS architecture knowledge.\nImproved personal programming skills.\nBecame familiar with NoSQL databases.\nLearned and practiced using Notion to manage schedules, store databases, and organize work.\n"},{"uri":"https://lhoanggg.github.io/intership-report/1-worklog/1.6-week6/","title":"Worklog Week 6","tags":[],"description":"","content":"Week 6 Objectives: Strengthen skills in using AWS CLI and SDK (Python/Boto3). Get familiar with AWS CloudFront and Lambda, understanding the basics of serverless architecture. Tasks to be implemented this week: Day Task Start Date End Date Reference Material 2 - Practice: + Create Access Key + Configure AWS CLI + Install Python and Boto3 13/10/2025 13/10/2025 https://000060.awsstudygroup.com/vi/3-gettingstartedwithawssdk/ 3 - Severe fever 14/10/2025 14/10/2025 4 - Severe fever 15/10/2025 15/10/2025 5 - Learn how to store web content on Amazon S3, protected and accelerated via Amazon CloudFront 16/10/2025 16/10/2025 https://000094.awsstudygroup.com/vi/ 6 - Attend AWS CloudFront workshops - Learn about Lambda 17/10/2025 17/10/2025 https://000130.awsstudygroup.com/vi/ Week 6 Achievements: Using AWS CLI and SDK\nCreated Access Key and configured AWS CLI. Understanding and deploying web storage with Amazon S3.\nPracticed AWS CloudFront and Lambda.\n"},{"uri":"https://lhoanggg.github.io/intership-report/1-worklog/1.7-week7/","title":"Worklog Week 7","tags":[],"description":"","content":"Week 7 Objectives: Optimize EC2 and Lambda costs. Improve skills in monitoring and managing systems using AWS CloudWatch. Complete AWS learning modules through Labs. Consolidate and systematize knowledge acquired in previous weeks. Tasks to be implemented this week: Day Task Start Date End Date Reference Material 2 - Learn how to optimize EC2 costs with Lambda - Practice advanced AWS CloudWatch Workshop 20/10/2025 20/10/2025 https://000022.awsstudygroup.com/vi/ 3 - Complete Module 1 - Module 1-Lab01 - Module 1-Lab07 - Module 1-Lab09 21/10/2025 21/10/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i 4 - Complete Module 2 - Module 2-Lab03 22/10/2025 22/10/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i 5 - Module 2-Lab10 - Module 2-Lab19 - Module 2-Lab20 23/10/2025 23/10/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i 6 - Consolidate all learned knowledge 24/10/2025 24/10/2025 Week 7 Achievements: Cost optimization and AWS system monitoring:\nLearned how to optimize EC2 costs in combination with Lambda. Practiced advanced AWS CloudWatch Workshop. AWS Lab skill completion:\nCompleted Module 1 and Module 2, including key Labs such as Lab01, Lab03, Lab07, Lab09, Lab10, Lab19, Lab20. Knowledge consolidation:\nReviewed and organized all previously learned concepts, ready for practical application. "},{"uri":"https://lhoanggg.github.io/intership-report/1-worklog/1.8-week8/","title":"Worklog Week 8","tags":[],"description":"","content":"Goals for Week 8: Review and consolidate all knowledge learned from previous weeks. Prepare knowledge for the midterm exam. Tasks to be carried out this week: Day Task Start Date End Date Reference Material 2 - Review and consolidate all learned knowledge 27/10/2025 27/10/2025 3 - Review learned knowledge 28/10/2025 28/10/2025 4 - Review learned knowledge 29/10/2025 29/10/2025 5 - Review learned knowledge 30/10/2025 30/10/2025 6 - Midterm Exam 31/10/2025 31/10/2025 Achievements for Week 8: Systematized and consolidated all learned knowledge.\nEffectively reviewed for the midterm exam.\n"},{"uri":"https://lhoanggg.github.io/intership-report/1-worklog/1.9-week9/","title":"Worklog Week 9","tags":[],"description":"","content":"Goals for Week 9: Assist the team in completing the project proposal. Research and select suitable services for the final project, optimizing costs. Review back-end code and enhance front-end skills with React. Complete and refine the team’s project architecture diagram. Tasks to be carried out this week: Day Task Start Date End Date Reference Material 2 - Assist teammates in making the proposal - Research services to be used in the final project 03/11/2025 03/11/2025 3 - Continue researching services for the final project to optimize costs - Learn proper AI prompting 04/11/2025 04/11/2025 4 - Review architecture drawing to complete and revise the team’s architecture diagram 05/11/2025 05/11/2025 5 - Review back-end code to prepare for project coding - Learn more about React 06/11/2025 06/11/2025 6 - Review services, review code, and continue learning React 07/11/2025 07/11/2025 Achievements for Week 9: Assisted the team in completing the proposal and planning the final project.\nUnderstood the necessary AWS/AI services for the project and optimized costs.\nReviewed and consolidated back-end coding skills.\nEnhanced front-end skills with React.\nCompleted and refined the team’s project architecture diagram.\n"},{"uri":"https://lhoanggg.github.io/intership-report/5-workshop/5.3-s3-vpc/5.3.2-test-gwe/","title":"Create Amazon RDS","tags":[],"description":"","content":"Step 1: Create RDS MySQL Instance Go to RDS Console → Databases → Create database\nChoose database creation method:\nStandard create Engine options:\nEngine type: MySQL Engine version: MySQL 8.0.35 (or latest 8.0.x) Templates:\nFree tier (for workshop/testing) Settings:\nDB instance identifier: daivietblood-db Master username: admin Credentials management: Self managed Master password: YourSecurePassword123! Confirm password: YourSecurePassword123! ⚠️ Important: Save your password securely. You will need it to connect from Lambda.\nStep 2: Instance Configuration Instance configuration: DB instance class: db.t3.micro (Free tier eligible) Storage type: General Purpose SSD (gp2) Allocated storage: 20 GiB Storage autoscaling: Disable (for cost control) Step 3: Connectivity Connectivity:\nCompute resource: Don\u0026rsquo;t connect to an EC2 compute resource Network type: IPv4 Virtual private cloud (VPC): daivietblood-vpc DB subnet group: daivietblood-db-subnet-group Public access: No ⚠️ Important! VPC security group: Choose existing Existing VPC security groups: daivietblood-rds-sg Availability Zone: ap-southeast-1a Database port:\nDatabase port: 3306 Step 4: Database Authentication Database authentication: Password authentication Step 5: Additional Configuration Database options:\nInitial database name: daivietblood DB parameter group: default.mysql8.0 Option group: default:mysql-8-0 Backup:\nEnable automated backups: Yes Backup retention period: 7 days Backup window: No preference Encryption:\nEnable encryption: Yes (default) Monitoring:\nEnable Enhanced monitoring: No (to reduce cost) Maintenance:\nEnable auto minor version upgrade: Yes Maintenance window: No preference Deletion protection:\nEnable deletion protection: No (for workshop) Click Create database\nℹ️ RDS creation takes 10-15 minutes. Wait until status shows \u0026ldquo;Available\u0026rdquo;.\nStep 6: Get RDS Endpoint After RDS is available:\nGo to RDS Console → Databases → Click daivietblood-db\nIn Connectivity \u0026amp; security tab, copy:\nEndpoint: daivietblood-db.xxxxxxxxxxxx.ap-southeast-1.rds.amazonaws.com Port: 3306 Save these values for Lambda configuration:\nDB_HOST=daivietblood-db.xxxxxxxxxxxx.ap-southeast-1.rds.amazonaws.com DB_PORT=3306 DB_NAME=daivietblood DB_USER=admin DB_PASSWORD=YourSecurePassword123! Step 7: Create Database Schema Since RDS is in Private Subnet, you need to connect via a bastion host or use Lambda to initialize the schema.\nOption A: Using Lambda to Initialize (Recommended)\nCreate a one-time Lambda function to initialize the database:\n// init-db.js const mysql = require(\u0026#39;mysql2/promise\u0026#39;); exports.handler = async (event) =\u0026gt; { const connection = await mysql.createConnection({ host: process.env.DB_HOST, user: process.env.DB_USER, password: process.env.DB_PASSWORD, database: process.env.DB_NAME }); // Create tables const createUsersTable = ` CREATE TABLE IF NOT EXISTS users ( id INT AUTO_INCREMENT PRIMARY KEY, email VARCHAR(255) UNIQUE NOT NULL, name VARCHAR(255) NOT NULL, blood_type ENUM(\u0026#39;A+\u0026#39;, \u0026#39;A-\u0026#39;, \u0026#39;B+\u0026#39;, \u0026#39;B-\u0026#39;, \u0026#39;AB+\u0026#39;, \u0026#39;AB-\u0026#39;, \u0026#39;O+\u0026#39;, \u0026#39;O-\u0026#39;) NOT NULL, phone VARCHAR(20), created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ) `; const createDonationsTable = ` CREATE TABLE IF NOT EXISTS donations ( id INT AUTO_INCREMENT PRIMARY KEY, user_id INT NOT NULL, donation_date DATE NOT NULL, location VARCHAR(255), status ENUM(\u0026#39;scheduled\u0026#39;, \u0026#39;completed\u0026#39;, \u0026#39;cancelled\u0026#39;) DEFAULT \u0026#39;scheduled\u0026#39;, created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP, FOREIGN KEY (user_id) REFERENCES users(id) ) `; const createEmergencyRequestsTable = ` CREATE TABLE IF NOT EXISTS emergency_requests ( id INT AUTO_INCREMENT PRIMARY KEY, requester_name VARCHAR(255) NOT NULL, blood_type ENUM(\u0026#39;A+\u0026#39;, \u0026#39;A-\u0026#39;, \u0026#39;B+\u0026#39;, \u0026#39;B-\u0026#39;, \u0026#39;AB+\u0026#39;, \u0026#39;AB-\u0026#39;, \u0026#39;O+\u0026#39;, \u0026#39;O-\u0026#39;) NOT NULL, units_needed INT NOT NULL, hospital VARCHAR(255) NOT NULL, urgency ENUM(\u0026#39;critical\u0026#39;, \u0026#39;urgent\u0026#39;, \u0026#39;normal\u0026#39;) DEFAULT \u0026#39;normal\u0026#39;, status ENUM(\u0026#39;open\u0026#39;, \u0026#39;fulfilled\u0026#39;, \u0026#39;cancelled\u0026#39;) DEFAULT \u0026#39;open\u0026#39;, created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ) `; await connection.execute(createUsersTable); await connection.execute(createDonationsTable); await connection.execute(createEmergencyRequestsTable); await connection.end(); return { statusCode: 200, body: JSON.stringify({ message: \u0026#39;Database initialized successfully\u0026#39; }) }; }; Verification Checklist RDS instance created and status is \u0026ldquo;Available\u0026rdquo; RDS is in Private Subnet (Public access: No) RDS Security Group only allows access from Lambda SG Endpoint and credentials saved securely Initial database daivietblood created Database schema initialized (tables created) Troubleshooting Issue Solution Cannot connect to RDS Verify Security Group allows inbound from Lambda SG RDS creation failed Check Service Quotas for RDS instances Connection timeout Ensure Lambda is in same VPC and has NAT Gateway access "},{"uri":"https://lhoanggg.github.io/intership-report/5-workshop/5.4-s3-onprem/5.4.2-create-interface-enpoint/","title":"Create API Gateway","tags":[],"description":"","content":"Step 1: Create REST API Go to API Gateway Console → Create API\nChoose API type:\nREST API → Build Create new API:\nProtocol: REST Create new API: New API API name: daivietblood-api Description: REST API for DaiVietBlood system Endpoint Type: Regional Click Create API\nStep 2: Create Resources 2.1. Create /users Resource\nSelect root / → Actions → Create Resource\nConfigure:\nResource Name: users Resource Path: users Enable API Gateway CORS: ✅ Check Click Create Resource\n2.2. Create /emergency-requests Resource\nSelect root / → Actions → Create Resource\nConfigure:\nResource Name: emergency-requests Resource Path: emergency-requests Enable API Gateway CORS: ✅ Check Click Create Resource\nStep 3: Create Methods for /users 3.1. GET /users\nSelect /users → Actions → Create Method → GET\nIntegration setup:\nIntegration type: Lambda Function Use Lambda Proxy integration: ✅ Check Lambda Region: ap-southeast-1 Lambda Function: daivietblood-get-users Click Save → OK (to add permission)\n3.2. POST /users\nSelect /users → Actions → Create Method → POST\nIntegration setup:\nIntegration type: Lambda Function Use Lambda Proxy integration: ✅ Check Lambda Region: ap-southeast-1 Lambda Function: daivietblood-create-user Click Save → OK\nStep 4: Create Methods for /emergency-requests 4.1. GET /emergency-requests\nSelect /emergency-requests → Actions → Create Method → GET\nIntegration setup:\nIntegration type: Lambda Function Use Lambda Proxy integration: ✅ Check Lambda Function: daivietblood-emergency-requests Click Save → OK\n4.2. POST /emergency-requests\nSelect /emergency-requests → Actions → Create Method → POST\nIntegration setup:\nIntegration type: Lambda Function Use Lambda Proxy integration: ✅ Check Lambda Function: daivietblood-emergency-requests Click Save → OK\nStep 5: Enable CORS For each resource (/users, /emergency-requests):\nSelect resource → Actions → Enable CORS\nConfigure:\nAccess-Control-Allow-Methods: GET, POST, OPTIONS Access-Control-Allow-Headers: Content-Type, X-Amz-Date, Authorization, X-Api-Key Access-Control-Allow-Origin: * Click Enable CORS and replace existing CORS headers\nClick Yes, replace existing values\nStep 6: Deploy API Actions → Deploy API\nDeployment stage:\nDeployment stage: [New Stage] Stage name: prod Stage description: Production stage Click Deploy\nCopy the Invoke URL:\nhttps://xxxxxxxxxx.execute-api.ap-southeast-1.amazonaws.com/prod ℹ️ Save this URL. You will need it for frontend configuration.\nStep 7: API Structure Summary After completing, your API structure should look like:\ndaivietblood-api │ ├── /users │ ├── GET → daivietblood-get-users │ ├── POST → daivietblood-create-user │ └── OPTIONS (CORS) │ └── /emergency-requests ├── GET → daivietblood-emergency-requests ├── POST → daivietblood-emergency-requests └── OPTIONS (CORS) Verification Checklist REST API created /users resource created with GET and POST methods /emergency-requests resource created with GET and POST methods CORS enabled for all resources API deployed to prod stage Invoke URL saved "},{"uri":"https://lhoanggg.github.io/intership-report/5-workshop/5.2-prerequiste/","title":"Preparation","tags":[],"description":"","content":"Prerequisites Before starting this workshop, ensure you have:\n1. AWS Account\nActive AWS Account with Administrator access Recommended: Use IAM User instead of Root account Region: Asia Pacific (Singapore) - ap-southeast-1 2. Local Development Tools\nTool Version Purpose Node.js \u0026gt;= 18.x Run Lambda functions locally npm/yarn Latest Package management AWS CLI \u0026gt;= 2.x Interact with AWS services Git Latest Version control 3. Knowledge Requirements\nBasic understanding of AWS services (VPC, EC2, S3) Familiarity with REST APIs Basic Node.js/JavaScript or Python Basic React knowledge Step 1: Configure AWS CLI Install AWS CLI from AWS CLI Installation Guide\nConfigure credentials:\naws configure Enter your credentials: AWS Access Key ID: [Your Access Key] AWS Secret Access Key: [Your Secret Key] Default region name: ap-southeast-1 Default output format: json Verify configuration: aws sts get-caller-identity Step 2: Create IAM User for Workshop Go to IAM Console → Users → Create user\nUser details:\nUser name: workshop-admin Select: Provide user access to the AWS Management Console Set permissions:\nSelect: Attach policies directly Search and select: AdministratorAccess Create user and save credentials securely\n⚠️ Security Note: After completing the workshop, delete this IAM user or remove AdministratorAccess policy.\nStep 3: Verify Service Quotas Ensure your account has sufficient quotas for:\nService Resource Minimum Required VPC VPCs per Region 1 VPC Subnets per VPC 4 VPC NAT Gateways per AZ 1 RDS DB Instances 1 Lambda Concurrent Executions 10 API Gateway REST APIs 1 S3 Buckets 2 Check quotas at: Service Quotas Console → Select service → View quotas\nStep 4: Prepare Source Code Clone the sample repository: git clone https://github.com/your-repo/daivietblood-workshop.git cd daivietblood-workshop Project structure: daivietblood-workshop/ ├── frontend/ # React application │ ├── src/ │ └── package.json ├── backend/ # Lambda functions │ ├── functions/ │ └── package.json ├── infrastructure/ # CloudFormation templates │ └── templates/ └── README.md Install dependencies: # Frontend cd frontend \u0026amp;\u0026amp; npm install # Backend cd ../backend \u0026amp;\u0026amp; npm install Step 5: Cost Estimation Service Configuration Est. Cost/Day NAT Gateway 1 NAT Gateway ~$1.08 RDS db.t3.micro ~$0.52 Lambda Free Tier $0.00 API Gateway Free Tier $0.00 S3 \u0026lt; 5GB ~$0.01 CloudFront \u0026lt; 1GB transfer ~$0.01 Amplify Build \u0026amp; Host ~$0.50 Total estimated: ~$2-3/day\n💡 Tip: Complete the workshop in 1-2 days and clean up resources immediately to minimize costs.\nChecklist Before Starting AWS Account ready with Administrator access AWS CLI installed and configured Node.js \u0026gt;= 18.x installed Git installed Source code cloned Region set to ap-southeast-1 "},{"uri":"https://lhoanggg.github.io/intership-report/2-proposal/","title":"Proposal","tags":[],"description":"","content":"Dai Viet Blood Donation \u0026amp; Emergency System (DaiVietBlood) Implemented by: Skyline Team – FPT University Ho Chi Minh City\nDate: December 7, 2025\nTABLE OF CONTENTS BACKGROUND AND MOTIVATION 1.1 Executive Summary 1.2 Project Success Criteria 1.3 Assumptions SOLUTION ARCHITECTURE / ARCHITECTURAL DIAGRAM 2.1 Technical Architecture Diagram 2.2 Technical Plan 2.3 Project Plan 2.4 Security Considerations ACTIVITIES AND DELIVERABLES 3.1 Activities and Deliverables 3.2 Out of Scope 3.3 Path to Production EXPECTED AWS COST BREAKDOWN IMPLEMENTATION TEAM RESOURCES \u0026amp; ESTIMATED PERSONNEL COSTS ACCEPTANCE 1. BACKGROUND AND MOTIVATION 1.1 EXECUTIVE SUMMARY Customer Background: The DaiVietBlood system is designed to serve the community, including voluntary blood donors, patients in need of emergency blood, and healthcare professionals in Vietnam. The primary customers are blood donors, patient families, and medical staff responsible for managing blood inventory and donation schedules. They require a centralized, reliable platform to optimize the matching process between donors and recipients and improve communication during emergencies. In the context of digital health transformation, DaiVietBlood provides a secure, accessible solution to address localized blood shortages.\nBusiness and Technical Objectives: Migrating the DaiVietBlood system from a local/on-premise environment to AWS offers superior advantages:\nBusiness: AWS allows the application to scale flexibly according to the user base, reduces hardware infrastructure operational costs, and ensures consistent performance nationwide. Technical: AWS provides High Availability and medical data security. Adopting a Serverless Architecture (AWS Lambda, API Gateway, Cognito, RDS) simplifies backend management, accelerates development, and reduces maintenance costs. The system integrates comprehensive monitoring (CloudWatch) and adheres to strict security standards. Summary of Key Use Cases:\nRole Key Function Short Description Guest Access Public Information View donation guidelines, blood compatibility charts, and educational articles without logging in. Member Register/Login, Profile Management Create accounts, update personal information and blood type. Book Blood Donation Select time slots and locations for donation. Submit Emergency Request Submit urgent blood requests; the system automatically finds suitable donors. Staff Manage Requests \u0026amp; Inventory Approve emergency requests, confirm donation schedules, update blood stock. Administrator (Admin) System Administration Manage user accounts, configure donation slots, view overview reports. Summary of Partner’s Professional Services: The Skyline Team will provide comprehensive digital transformation services, including assessing the current local application, designing a Cloud-native architecture, and executing the migration of the system to an AWS Serverless environment. We commit to delivering a secure, scalable system accompanied by automated CI/CD pipelines and detailed operational documentation.\n1.2 PROJECT SUCCESS CRITERIA Functionality: 100% of core functions (registration, scheduling, emergency requests, administration) operate stably on AWS with no regression errors. Availability: System achieves Uptime ≥ 99.9%, ensuring continuous 24/7 access. Performance: Application response time improves by at least 30% compared to the local version. Emergency request processing time is reduced by 40%. Cost: Infrastructure costs are optimized by at least 20% thanks to the Serverless model and Auto-scaling. User Experience: UAT acceptance rate reaches a minimum of 95% for all user roles. Security: Full compliance with data encryption, access management (IAM), and API security requirements. Operations: CI/CD pipeline is fully automated with deployment time \u0026lt; 10 minutes. Monitoring system covers 100% of critical services. 1.3 ASSUMPTIONS The project is implemented based on the following assumptions. If changes occur, the scope and schedule may need adjustment:\nTechnical \u0026amp; Architectural Assumptions:\nSource Code: The current Local application (Frontend \u0026amp; Backend) is functionally complete. The project focuses on Refactoring for the Cloud (Serverless), excluding the development of new features. AWS Region: The entire infrastructure is deployed in Singapore (ap-southeast-1) to optimize latency for users in Vietnam. Note: During the testing phase, due to limited VPC configurations and Free Tier/Student resources, latency may fluctuate (estimated ~3.5s/request). Service Limits: The AWS account uses default limits (Soft limits). Increasing limits to reduce latency will be approved by the Customer when necessary. Third-party Integration: The system uses the Gemini API for AI support features. Access Rights: The Skyline Team is granted Admin access (IAM Role) to provision resources. Operational \u0026amp; Financial Assumptions:\nDomain: The Customer owns the domain name (e.g., daivietblood.com) and DNS configuration rights. Cost: The cost estimate is based on an assumption of approximately 50,000 API requests/month. Actual costs depend on usage levels (Pay-as-you-go). 2. SOLUTION ARCHITECTURE / ARCHITECTURAL DIAGRAM 2.1 TECHNICAL ARCHITECTURE DIAGRAM The DaiVietBlood system utilizes a Serverless-First architecture on AWS Cloud, prioritizing scalability, security, and operational optimization.\n(Refer to the architecture diagram provided in the original document)\nKey Components:\nNetwork Infrastructure (VPC): Public Subnet: Contains Internet Gateway and NAT Gateway. Private Subnet: Contains AWS Lambda and Amazon RDS to isolate and secure data, preventing direct Internet access. Application \u0026amp; Data: Frontend: Hosted on AWS Amplify, distributed via Amazon CloudFront (CDN), and assets stored on S3. Authentication: Amazon Cognito manages identity and issues JWT tokens. API \u0026amp; Compute: Amazon API Gateway receives requests and routes them to AWS Lambda for business logic processing. Database: Amazon RDS stores structured data, located in the Private Subnet. DevOps \u0026amp; Monitoring: CI/CD: Uses AWS CodePipeline, CodeBuild, CodeDeploy to automate the deployment process. Monitoring: Amazon CloudWatch centrally collects logs and metrics. 2.2 TECHNICAL PLAN The technical implementation process follows the Infrastructure-as-Code (IaC) methodology:\nInfrastructure Automation: Use AWS CloudFormation to provision VPC, Lambda, RDS, and API Gateway, ensuring consistency across environments (Dev/Staging/Prod). Application Development: Refactor backend into modular Lambda functions (NodeJS/Python). Environment variables and sensitive information (DB credentials) are securely encrypted. CI/CD Process: Source (GitHub) -\u0026gt; Build (CodeBuild) -\u0026gt; Deploy (CloudFormation/CodeDeploy). Includes a Manual Approval step before deploying to the Production environment. Testing Strategy: Unit Tests for Lambda, Integration Tests for API, and Load Tests to ensure capacity. 2.3 PROJECT PLAN The project applies the Agile Scrum model over 8 weeks (4 Sprints):\nSprint 1 (Foundation): Set up AWS Account, VPC, RDS. Sprint 2 (Backend Core): Develop Lambda, API Gateway, Cognito. Sprint 3 (Integration): Deploy Frontend (Amplify), finalize CI/CD Pipeline. Sprint 4 (Stabilization): UAT, Performance Optimization, Handover. 2.4 SECURITY CONSIDERATIONS Access Management: Use Cognito for user authentication and IAM Roles for service authorization (Least Privilege). Network Isolation: Database and Lambda are located in the Private Subnet, accessing the Internet only via NAT Gateway. Data Protection: Data encryption At-rest (on RDS/S3) and In-transit (via HTTPS). Security Monitoring: CloudWatch Logs record all activities for auditing and intrusion detection. 3. ACTIVITIES AND DELIVERABLES 3.1 ACTIVITIES AND DELIVERABLES Phase Timeline Key Activities Deliverables Estimate (Man-days) Analysis \u0026amp; Design Week 1 Assess Local state, design Cloud architecture, plan migration. SRS Document, Architecture Diagram, API Specs. 5 Local Development Week 2-3 Build backend logic, database schema, local unit tests. Backend Prototype, Database Schema. 10 Frontend \u0026amp; Integration Week 4-5 Develop Frontend, integrate local APIs, prepare code for refactoring. Completed Local Application. 10 AWS Infrastructure Setup Week 6 Write CloudFormation scripts, provision VPC, RDS, IAM. IaC Templates, Secure VPC Environment. 5 Refactor \u0026amp; Deploy Backend Week 7-8 Convert to Lambda, configure API Gateway, Cognito. Serverless Backend active on AWS. 10 Deploy Frontend \u0026amp; CI/CD Week 9-10 Host Frontend on Amplify, set up automated Pipeline. Production URL, CI/CD Pipeline. 10 Testing \u0026amp; Go-live Week 11 UAT, Security Testing, Performance Optimization. UAT Report, Security Report. 5 Handover \u0026amp; Training Week 12 Transfer accounts, operations training, handover documentation. Operations Manual, Acceptance. 5 3.2 OUT OF SCOPE (MVP PHASE) Due to time and resource limitations of the MVP phase, the following items are not included:\nOptimal user search algorithm based on real-time Geo-location (currently using simplified logic). Complex Auto-scaling for the Database layer (currently using basic RDS). Deep Latency Optimization for regions outside Singapore. Advanced Security Compliance standards such as HIPAA/PCI-DSS. 3.3 PATH TO PRODUCTION To upgrade from the current MVP to a large-scale Production system, the following are required:\nEnvironment Strategy: Strictly separate Dev/Staging/Prod environments across different AWS accounts (Multi-account strategy). Database Scaling: Migrate to Amazon Aurora Serverless or use Read Replicas to increase read/write capacity. Enhanced Monitoring: Integrate AWS X-Ray to trace requests and identify performance bottlenecks. Strengthened Security: Deploy AWS WAF with rules to block DDoS and automated bots; use Amazon Inspector for periodic vulnerability scanning. 4. EXPECTED AWS COST BREAKDOWN Region: Asia Pacific (Singapore)\nCategory Service Estimated Configuration Monthly Cost (USD) Network NAT Gateway 1 NAT Gateway (Required for Private Subnet) + Data Processing ~$43.13 VPC Subnets, Security Groups ~$13.14 CloudFront 5GB Data Transfer (Utilizing Free Tier) ~$3.00 Compute Lambda 1,000 requests, 512MB RAM (Free Tier) ~$0.00 API Gateway 1,000 requests ~$0.00 Database RDS db.t3.micro, 20GB Storage ~$21.74 Storage S3 5GB Storage, 200 requests ~$0.14 Hosting Amplify Build \u0026amp; Hosting, WAF enabled ~$16.77 Ops CloudWatch Logs, Metrics, Alarms ~$9.41 CI/CD CodePipeline 1 Active Pipeline ~$1.05 Total ~$108.38 / Month Note: Actual costs may be lower thanks to the Free Tier (first 12 months) and AWS Credits for students.\n5. IMPLEMENTATION TEAM Mentor (AWS FCJ): Nguyen Gia Hung - Head of Solutions Architect. Project Manager (PM): Nguyen Duc Lan - Coordination, schedule management, cost optimization, and UAT strategy. Technical Lead: Nguyen Cong Minh - In charge of CI/CD, Infrastructure (CDK), Security, and Lambda. Solution Architect: Do Khang - Serverless architecture design, AI Chatbot integration, Service Policies. Fullstack Developer: Le Hoang Anh - API development, Frontend UI/UX, and application security. Data Engineer: Nguyen Quach Lam Giang - RDS administration, VPC/Subnet design, and CloudWatch monitoring. 6. RESOURCES \u0026amp; ESTIMATED PERSONNEL COSTS Time Allocation (Man-hours): The project mobilizes a total of 750 man-hours distributed equally among 5 members across phases: Foundation, Core Development, Data Analysis, Testing, and Handover.\nCost Allocation:\nPersonnel: $0 (Performed by students as part of an internship/capstone project, counted towards academic credits). AWS Infrastructure: ~$15 (Actual costs incurred during dev/test after deducting Credits). Total Project Cost: Highly optimized, relying mainly on internal resources and support from the AWS FCJ program. 7. ACCEPTANCE 7.1 Submission of Deliverables: Upon completion of the \u0026ldquo;Handover\u0026rdquo; phase, the Skyline Team will submit all Source Code, Architecture Documentation, Admin Accounts, and Operations Manuals to the Customer/Mentor.\n7.2 Acceptance Period \u0026amp; Process: The Customer has 05 business days to review and perform UAT. If the product meets the Success Criteria (Section 1.2), the Customer will sign the acceptance confirmation.\n7.3 Defect Remediation: If critical errors arise or features are missing compared to the committed scope, the Skyline Team is responsible for fixing and resubmitting for acceptance as soon as possible.\n"},{"uri":"https://lhoanggg.github.io/intership-report/5-workshop/5.4-s3-onprem/5.4.3-test-endpoint/","title":"Test API Endpoints","tags":[],"description":"","content":"Step 1: Test from API Gateway Console 1.1. Test GET /users\nGo to API Gateway Console → Select daivietblood-api Select /users → GET Click Test Click Test button Expected response:\n{ \u0026#34;statusCode\u0026#34;: 200, \u0026#34;body\u0026#34;: \u0026#34;[]\u0026#34; } Step 2: Test with cURL Replace YOUR_API_URL with your actual Invoke URL.\n2.1. Create a User (POST /users)\ncurl -X POST https://YOUR_API_URL/prod/users \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;email\u0026#34;: \u0026#34;nguyen.van.a@example.com\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Nguyen Van A\u0026#34;, \u0026#34;blood_type\u0026#34;: \u0026#34;O+\u0026#34;, \u0026#34;phone\u0026#34;: \u0026#34;0901234567\u0026#34; }\u0026#39; Expected response:\n{ \u0026#34;id\u0026#34;: 1, \u0026#34;email\u0026#34;: \u0026#34;nguyen.van.a@example.com\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Nguyen Van A\u0026#34;, \u0026#34;blood_type\u0026#34;: \u0026#34;O+\u0026#34;, \u0026#34;phone\u0026#34;: \u0026#34;0901234567\u0026#34; } 2.2. Get All Users (GET /users)\ncurl https://YOUR_API_URL/prod/users Expected response:\n[ { \u0026#34;id\u0026#34;: 1, \u0026#34;email\u0026#34;: \u0026#34;nguyen.van.a@example.com\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Nguyen Van A\u0026#34;, \u0026#34;blood_type\u0026#34;: \u0026#34;O+\u0026#34;, \u0026#34;phone\u0026#34;: \u0026#34;0901234567\u0026#34;, \u0026#34;created_at\u0026#34;: \u0026#34;2025-12-09T10:00:00.000Z\u0026#34; } ] 2.3. Create Emergency Request (POST /emergency-requests)\ncurl -X POST https://YOUR_API_URL/prod/emergency-requests \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;requester_name\u0026#34;: \u0026#34;Benh vien Cho Ray\u0026#34;, \u0026#34;blood_type\u0026#34;: \u0026#34;AB-\u0026#34;, \u0026#34;units_needed\u0026#34;: 5, \u0026#34;hospital\u0026#34;: \u0026#34;Cho Ray Hospital\u0026#34;, \u0026#34;urgency\u0026#34;: \u0026#34;critical\u0026#34; }\u0026#39; Expected response:\n{ \u0026#34;id\u0026#34;: 1, \u0026#34;message\u0026#34;: \u0026#34;Emergency request created\u0026#34; } 2.4. Get Emergency Requests (GET /emergency-requests)\ncurl https://YOUR_API_URL/prod/emergency-requests Step 3: Test with Postman Open Postman Create new Collection: DaiVietBlood API Add requests: Request Name Method URL Get Users GET {{baseUrl}}/users Create User POST {{baseUrl}}/users Get Emergency Requests GET {{baseUrl}}/emergency-requests Create Emergency Request POST {{baseUrl}}/emergency-requests Set Collection variable: baseUrl: https://YOUR_API_URL/prod Step 4: Verify Lambda Logs Go to CloudWatch Console → Log groups\nFind log groups:\n/aws/lambda/daivietblood-get-users /aws/lambda/daivietblood-create-user /aws/lambda/daivietblood-emergency-requests Check recent log streams for:\nSuccessful invocations Any errors or exceptions Database connection logs Common Issues \u0026amp; Solutions Issue Cause Solution 502 Bad Gateway Lambda error Check CloudWatch logs for details Timeout Lambda cannot reach RDS Verify VPC, Subnets, Security Groups CORS error CORS not configured Enable CORS on API Gateway 500 Internal Server Error Database connection failed Check DB credentials in environment variables Step 5: Performance Check Note the response time for each API call First call may be slow (Lambda cold start) Subsequent calls should be faster Expected performance:\nEndpoint Cold Start Warm GET /users ~3-5s ~200-500ms POST /users ~3-5s ~200-500ms GET /emergency-requests ~3-5s ~200-500ms 💡 Tip: Lambda cold start in VPC can be slow. Consider using Provisioned Concurrency for production workloads.\nVerification Checklist GET /users returns empty array or user list POST /users creates new user successfully GET /emergency-requests returns requests list POST /emergency-requests creates new request No CORS errors in browser console CloudWatch logs show successful invocations "},{"uri":"https://lhoanggg.github.io/intership-report/5-workshop/5.3-s3-vpc/","title":"VPC &amp; Amazon RDS","tags":[],"description":"","content":"In this section, you will create the network infrastructure (VPC) and database (RDS) for the DaiVietBlood system.\nArchitecture Overview Content Create VPC Create Amazon RDS "},{"uri":"https://lhoanggg.github.io/intership-report/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":"This section will list and introduce the blogs you have translated. For example:\nBlog 1 - Monitoring server health with Amazon GameLift Servers This blog explains how to monitor and diagnose multiplayer game server performance using Amazon GameLift. It emphasizes leveraging telemetry metrics (CPU, memory, network, tick rate, and custom metrics) along with Amazon Managed Grafana to detect crashes, bottlenecks, and resource issues. The article also guides troubleshooting scenarios such as game session crashes, high CPU usage, and container support, while extending monitoring with custom metrics and proactive alerts to ensure a smooth player experience.\nBlog 2 - Advanced analytics using Amazon CloudWatch Logs Insights This blog introduces the new advanced analytics capabilities of Amazon CloudWatch Logs Insights, enabling users to analyze log data more effectively using SQL syntax and the Piped Processing Language (PPL). It highlights enhanced abilities such as performing JOINs across multiple log groups, executing subqueries, running statistical functions, and parsing complex JSON structures. The blog also presents AI-powered features that simplify log analysis, including natural language query generation, automated query result summarization, and on-demand anomaly detection. These capabilities help reduce investigation time, accelerate debugging, and improve overall system observability. Additionally, the article showcases real-world use cases such as application error analysis, performance issue detection, correlating logs across microservices, and building end-to-end log analytics workflows on CloudWatch.\nBlog 3 - Handling sensitive log data using Amazon CloudWatch This blog explains how to protect sensitive data in logs using CloudWatch Logs: by applying data protection policies that automatically detect and mask/redact sensitive fields (PII, financial data, health info) at ingestion. CloudWatch offers many built-in “managed data identifiers” and allows custom identifiers. Only users or roles with logs:Unmask permission can view raw data — balancing data privacy with observability and debugging capabilities.\n"},{"uri":"https://lhoanggg.github.io/intership-report/5-workshop/5.4-s3-onprem/5.4.4-dns-simulation/","title":"Configure CORS &amp; Security","tags":[],"description":"","content":"Understanding CORS CORS (Cross-Origin Resource Sharing) is a security feature that restricts web pages from making requests to a different domain than the one serving the web page.\nWhen your React frontend (hosted on Amplify) calls your API Gateway, the browser checks CORS headers to determine if the request is allowed.\nStep 1: Configure CORS Headers in Lambda Ensure all Lambda functions return proper CORS headers:\nconst corsHeaders = { \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39;, // Or specific domain \u0026#39;Access-Control-Allow-Headers\u0026#39;: \u0026#39;Content-Type,X-Amz-Date,Authorization,X-Api-Key,X-Amz-Security-Token\u0026#39;, \u0026#39;Access-Control-Allow-Methods\u0026#39;: \u0026#39;GET,POST,PUT,DELETE,OPTIONS\u0026#39; }; // In your handler response: return { statusCode: 200, headers: corsHeaders, body: JSON.stringify(data) }; Step 2: Configure API Gateway CORS Method 1: Using Console\nGo to API Gateway Console → Select your API For each resource: Select resource → Actions → Enable CORS Configure allowed origins, methods, headers Click Enable CORS and replace existing CORS headers Method 2: Using OPTIONS Method\nCreate OPTIONS method for each resource Integration type: Mock Add Method Response with status 200 Add Integration Response with headers: Access-Control-Allow-Headers: \u0026#39;Content-Type,X-Amz-Date,Authorization,X-Api-Key\u0026#39; Access-Control-Allow-Methods: \u0026#39;GET,POST,OPTIONS\u0026#39; Access-Control-Allow-Origin: \u0026#39;*\u0026#39; Step 3: API Gateway Security Best Practices 3.1. Enable API Key (Optional)\nGo to API Gateway → API Keys → Create API Key Name: daivietblood-api-key Go to Usage Plans → Create Configure throttling and quota Associate API Key with Usage Plan For each method, set API Key Required: true 3.2. Enable Request Validation\nGo to API Gateway → Models → Create Create model for request body: { \u0026#34;$schema\u0026#34;: \u0026#34;http://json-schema.org/draft-04/schema#\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;CreateUserModel\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;required\u0026#34;: [\u0026#34;email\u0026#34;, \u0026#34;name\u0026#34;, \u0026#34;blood_type\u0026#34;], \u0026#34;properties\u0026#34;: { \u0026#34;email\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;email\u0026#34; }, \u0026#34;name\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;minLength\u0026#34;: 1 }, \u0026#34;blood_type\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;enum\u0026#34;: [\u0026#34;A+\u0026#34;, \u0026#34;A-\u0026#34;, \u0026#34;B+\u0026#34;, \u0026#34;B-\u0026#34;, \u0026#34;AB+\u0026#34;, \u0026#34;AB-\u0026#34;, \u0026#34;O+\u0026#34;, \u0026#34;O-\u0026#34;] }, \u0026#34;phone\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; } } } Apply model to POST method: Select method → Method Request Request Validator: Validate body Request Body: Add model 3.3. Enable Throttling\nGo to Stages → Select prod Stage Settings → Default Method Throttling Configure: Rate: 100 requests/second Burst: 200 requests Step 4: Lambda Security Best Practices 4.1. Use AWS Secrets Manager for Credentials\nInstead of storing DB credentials in environment variables:\nGo to Secrets Manager → Store a new secret\nSecret type: Other type of secret\nKey/value pairs:\nDB_HOST: daivietblood-db.xxxx.rds.amazonaws.com DB_USER: admin DB_PASSWORD: YourSecurePassword123! DB_NAME: daivietblood Secret name: daivietblood/db-credentials\nUpdate Lambda to retrieve secrets:\nconst { SecretsManagerClient, GetSecretValueCommand } = require(\u0026#39;@aws-sdk/client-secrets-manager\u0026#39;); const client = new SecretsManagerClient({ region: \u0026#39;ap-southeast-1\u0026#39; }); const getDbCredentials = async () =\u0026gt; { const command = new GetSecretValueCommand({ SecretId: \u0026#39;daivietblood/db-credentials\u0026#39; }); const response = await client.send(command); return JSON.parse(response.SecretString); }; Add IAM permission to Lambda role: { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;secretsmanager:GetSecretValue\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:secretsmanager:ap-southeast-1:*:secret:daivietblood/*\u0026#34; } 4.2. Input Validation\nAlways validate input in Lambda:\nconst validateUser = (body) =\u0026gt; { const errors = []; if (!body.email || !isValidEmail(body.email)) { errors.push(\u0026#39;Invalid email\u0026#39;); } if (!body.name || body.name.length \u0026lt; 1) { errors.push(\u0026#39;Name is required\u0026#39;); } const validBloodTypes = [\u0026#39;A+\u0026#39;, \u0026#39;A-\u0026#39;, \u0026#39;B+\u0026#39;, \u0026#39;B-\u0026#39;, \u0026#39;AB+\u0026#39;, \u0026#39;AB-\u0026#39;, \u0026#39;O+\u0026#39;, \u0026#39;O-\u0026#39;]; if (!validBloodTypes.includes(body.blood_type)) { errors.push(\u0026#39;Invalid blood type\u0026#39;); } return errors; }; Step 5: Redeploy API After making changes:\nActions → Deploy API Select prod stage Click Deploy Security Checklist CORS configured correctly Lambda returns proper CORS headers API Key enabled (optional but recommended) Request validation enabled Throttling configured DB credentials stored in Secrets Manager (recommended) Input validation in Lambda functions API redeployed after changes "},{"uri":"https://lhoanggg.github.io/intership-report/4-eventparticipated/","title":"Events Participated","tags":[],"description":"","content":" In this section, you should list and describe in detail the events you have participated in during your internship or work experience.\nEach event should be presented in the format Event 1, Event 2, Event 3…, along with the following details:\nEvent name Date and time Location (if applicable) Your role in the event (attendee, event support, speaker, etc.) A brief description of the event’s content and main activities Outcomes or value gained (lessons learned, new skills, contribution to the team/project) This listing helps demonstrate your actual participation as well as the soft skills and experience you have gained from each event. During my internship, I participated in two events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments.\nEvent 1 Event Name: Cloud Day Vietnam\nDate \u0026amp; Time: 09:00, September 18, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 2 Event Name: Data Resiliency in a Cloud - First World\nDate \u0026amp; Time: 09:00, October 14, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 3 Event Name: Data Science on AWS\nDate \u0026amp; Time: 09:00, October 16, 2025\nLocation: Hall B, FPT University Ho Chi Minh, District 9, Thu Duc, Ho Chi Minh\nRole: Attendee\nEvent 4 Event Name: AWS Cloud Mastery Series #1\nDate \u0026amp; Time: 09:00, November 15, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 5 Event Name: AWS Cloud Mastery Series #2\nDate \u0026amp; Time: 09:00, November 17, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 6 Event Name: AWS Cloud Mastery Series #3\nDate \u0026amp; Time: 09:00, November 29, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\n"},{"uri":"https://lhoanggg.github.io/intership-report/5-workshop/5.4-s3-onprem/","title":"Lambda &amp; API Gateway","tags":[],"description":"","content":"In this section, you will create AWS Lambda functions and expose them via Amazon API Gateway to build the serverless backend for DaiVietBlood.\nArchitecture Overview API Endpoints Method Endpoint Description GET /users Get all users POST /users Create new user GET /users/{id} Get user by ID GET /donations Get all donations POST /donations Create donation appointment GET /emergency-requests Get emergency requests POST /emergency-requests Create emergency request Content Create Lambda Functions Create API Gateway Test API Endpoints Configure CORS \u0026amp; Security "},{"uri":"https://lhoanggg.github.io/intership-report/5-workshop/5.5-policy/","title":"S3, CloudFront &amp; Amplify","tags":[],"description":"","content":"In this section, you will set up Amazon S3 for static assets, CloudFront for content distribution, and AWS Amplify to host the React frontend application.\nArchitecture Overview Part 1: Amazon S3 Setup Step 1: Create S3 Bucket for Assets Go to S3 Console → Create bucket\nGeneral configuration:\nBucket name: daivietblood-assets-{your-account-id} AWS Region: Asia Pacific (Singapore) ap-southeast-1 Object Ownership:\nACLs disabled (recommended) Block Public Access settings:\nBlock all public access: ✅ (We\u0026rsquo;ll use CloudFront) Bucket Versioning:\nEnable (recommended for production) Default encryption:\nServer-side encryption: Enable Encryption type: Amazon S3 managed keys (SSE-S3) Click Create bucket\nStep 2: Create Bucket Policy for CloudFront After creating CloudFront distribution (Part 2), update bucket policy:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;AllowCloudFrontAccess\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;cloudfront.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;s3:GetObject\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::daivietblood-assets-{your-account-id}/*\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;AWS:SourceArn\u0026#34;: \u0026#34;arn:aws:cloudfront::{account-id}:distribution/{distribution-id}\u0026#34; } } } ] } Step 3: Upload Sample Assets Create folder structure:\n/images /blood-types /icons /banners /documents Upload sample images for the application\nPart 2: CloudFront Setup Step 1: Create CloudFront Distribution Go to CloudFront Console → Create distribution\nOrigin settings:\nOrigin domain: Select your S3 bucket Origin path: Leave empty Name: daivietblood-s3-origin Origin access: Origin access control settings (recommended) Create new OAC: Click Create control setting Name: daivietblood-oac Signing behavior: Sign requests Default cache behavior:\nViewer protocol policy: Redirect HTTP to HTTPS Allowed HTTP methods: GET, HEAD Cache policy: CachingOptimized Settings:\nPrice class: Use only North America and Europe (or All edge locations) Default root object: index.html Click Create distribution\nImportant: Copy the bucket policy provided and update your S3 bucket policy\nStep 2: Get CloudFront Domain After distribution is deployed (takes 5-10 minutes):\nCopy the Distribution domain name:\nhttps://d1234567890.cloudfront.net Test accessing an asset:\nhttps://d1234567890.cloudfront.net/images/logo.png Part 3: AWS Amplify Setup Step 1: Prepare React Application Create React app (if not exists): npx create-react-app daivietblood-frontend cd daivietblood-frontend Install dependencies: npm install axios react-router-dom Create .env file: REACT_APP_API_URL=https://xxxxxxxxxx.execute-api.ap-southeast-1.amazonaws.com/prod REACT_APP_ASSETS_URL=https://d1234567890.cloudfront.net Sample API service (src/services/api.js): import axios from \u0026#39;axios\u0026#39;; const API_URL = process.env.REACT_APP_API_URL; export const getUsers = async () =\u0026gt; { const response = await axios.get(`${API_URL}/users`); return response.data; }; export const createUser = async (userData) =\u0026gt; { const response = await axios.post(`${API_URL}/users`, userData); return response.data; }; export const getEmergencyRequests = async () =\u0026gt; { const response = await axios.get(`${API_URL}/emergency-requests`); return response.data; }; export const createEmergencyRequest = async (requestData) =\u0026gt; { const response = await axios.post(`${API_URL}/emergency-requests`, requestData); return response.data; }; Push to GitHub repository Step 2: Deploy with Amplify Go to AWS Amplify Console → Create new app\nChoose source:\nGitHub → Continue Authorize AWS Amplify to access your GitHub Add repository branch:\nRepository: Select your repository Branch: main Configure build settings:\nApp name: daivietblood-frontend Build and test settings: Auto-detected for React Build settings (amplify.yml):\nversion: 1 frontend: phases: preBuild: commands: - npm ci build: commands: - npm run build artifacts: baseDirectory: build files: - \u0026#39;**/*\u0026#39; cache: paths: - node_modules/**/* Environment variables:\nAdd REACT_APP_API_URL and REACT_APP_ASSETS_URL Click Save and deploy\nStep 3: Configure Custom Domain (Optional) Go to App settings → Domain management Click Add domain Enter your domain name Configure DNS records as instructed Part 4: Verify Deployment Access Amplify URL:\nhttps://main.d1234567890.amplifyapp.com Test functionality:\nHomepage loads correctly API calls work (check Network tab) Images load from CloudFront No CORS errors Verification Checklist S3 bucket created with proper settings CloudFront distribution deployed S3 bucket policy updated for CloudFront access Assets accessible via CloudFront URL React app deployed to Amplify Environment variables configured Frontend can call API Gateway Images load from CloudFront "},{"uri":"https://lhoanggg.github.io/intership-report/5-workshop/","title":"Workshop","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nBuilding Serverless System on AWS - DaiVietBlood Overview This workshop guides you through building a Serverless Blood Donation \u0026amp; Emergency System (DaiVietBlood) on AWS. You will learn how to set up and configure the core AWS services used in the project architecture.\nAWS Services Used Service Purpose Amazon VPC Create virtual private network with Public/Private Subnets NAT Gateway Allow resources in Private Subnet to access Internet Amazon RDS MySQL database for the application AWS Lambda Serverless business logic processing Amazon API Gateway Manage and expose REST APIs Amazon S3 Store static assets (images, files) Amazon CloudFront CDN for global content distribution AWS Amplify Host Frontend application (React) AWS CodePipeline CI/CD automation Amazon CloudWatch Monitoring and logging What You Will Learn Design and deploy Serverless-First architecture on AWS Configure VPC with Public/Private Subnets for security Create RDS MySQL in Private Subnet Build Lambda Functions and connect with API Gateway Store and distribute content with S3 and CloudFront Deploy React application with AWS Amplify Set up automated CI/CD Pipeline Monitor application with CloudWatch Prerequisites AWS Account with Administrator access Basic knowledge of AWS services Familiarity with Node.js and React AWS CLI installed and configured Estimated Cost This workshop uses resources within AWS Free Tier when possible. Estimated cost is approximately ~$15-20 if completed within 1-2 days and resources are cleaned up immediately after.\nContent Workshop Overview Preparation VPC \u0026amp; Amazon RDS Lambda \u0026amp; API Gateway S3, CloudFront \u0026amp; Amplify CI/CD, CloudWatch \u0026amp; Cleanup "},{"uri":"https://lhoanggg.github.io/intership-report/5-workshop/5.6-cleanup/","title":"CI/CD, CloudWatch &amp; Cleanup","tags":[],"description":"","content":"In this final section, you will set up CI/CD Pipeline, configure CloudWatch monitoring, and clean up all resources after completing the workshop.\nPart 1: CI/CD Pipeline with CodePipeline Step 1: Create CodeBuild Project Go to CodeBuild Console → Create build project\nProject configuration:\nProject name: daivietblood-backend-build Description: Build project for Lambda functions Source:\nSource provider: GitHub Repository: Select your repository Branch: main Environment:\nEnvironment image: Managed image Operating system: Amazon Linux 2 Runtime: Standard Image: aws/codebuild/amazonlinux2-x86_64-standard:4.0 Service role: New service role Buildspec:\nBuild specifications: Use a buildspec file Create buildspec.yml file in your repository: version: 0.2 phases: install: runtime-versions: nodejs: 18 commands: - echo Installing dependencies... - cd backend \u0026amp;\u0026amp; npm ci pre_build: commands: - echo Running tests... - npm test || true build: commands: - echo Building Lambda packages... - mkdir -p dist - zip -r dist/get-users.zip functions/get-users/ - zip -r dist/create-user.zip functions/create-user/ - zip -r dist/emergency-requests.zip functions/emergency-requests/ post_build: commands: - echo Updating Lambda functions... - aws lambda update-function-code --function-name daivietblood-get-users --zip-file fileb://dist/get-users.zip - aws lambda update-function-code --function-name daivietblood-create-user --zip-file fileb://dist/create-user.zip - aws lambda update-function-code --function-name daivietblood-emergency-requests --zip-file fileb://dist/emergency-requests.zip artifacts: files: - dist/**/* Click Create build project Step 2: Create CodePipeline Go to CodePipeline Console → Create pipeline\nPipeline settings:\nPipeline name: daivietblood-pipeline Service role: New service role Source stage:\nSource provider: GitHub (Version 2) Connection: Create new connection or select existing Repository name: Select your repository Branch name: main Output artifact format: CodePipeline default Build stage:\nBuild provider: AWS CodeBuild Project name: daivietblood-backend-build Deploy stage:\nSkip deploy stage (Lambda is updated in build stage) Click Create pipeline\nStep 3: Add IAM Permissions for CodeBuild Go to IAM Console → Roles Find role codebuild-daivietblood-backend-build-service-role Add inline policy: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;lambda:UpdateFunctionCode\u0026#34;, \u0026#34;lambda:GetFunction\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:lambda:ap-southeast-1:*:function:daivietblood-*\u0026#34; } ] } Part 2: CloudWatch Monitoring Step 1: Create CloudWatch Dashboard Go to CloudWatch Console → Dashboards → Create dashboard\nDashboard name: DaiVietBlood-Monitoring\nAdd widgets:\nWidget 1: Lambda Invocations\nWidget type: Line Metrics: Lambda → By Function Name → Invocations Select all daivietblood functions Widget 2: Lambda Errors\nWidget type: Number Metrics: Lambda → By Function Name → Errors Statistic: Sum Widget 3: Lambda Duration\nWidget type: Line Metrics: Lambda → By Function Name → Duration Statistic: Average Widget 4: API Gateway Requests\nWidget type: Line Metrics: ApiGateway → By Api Name → Count Widget 5: RDS Connections\nWidget type: Line Metrics: RDS → Per-Database Metrics → DatabaseConnections Step 2: Create CloudWatch Alarms Alarm 1: Lambda Errors\nGo to CloudWatch → Alarms → Create alarm Select metric: Lambda → By Function Name → Errors Conditions: Threshold type: Static Whenever Errors is: Greater than 5 Period: 5 minutes Notification: Create new SNS topic: daivietblood-alerts Email: your-email@example.com Alarm name: DaiVietBlood-Lambda-Errors Alarm 2: RDS CPU High\nCreate alarm Select metric: RDS → Per-Database Metrics → CPUUtilization Conditions: Threshold: Greater than 80% Period: 5 minutes Notification: Use existing SNS topic Alarm name: DaiVietBlood-RDS-CPU-High Alarm 3: API Gateway 5XX Errors\nCreate alarm Select metric: ApiGateway → By Api Name → 5XXError Conditions: Threshold: Greater than 10 Period: 5 minutes Alarm name: DaiVietBlood-API-5XX-Errors Step 3: Configure Log Insights Go to CloudWatch → Logs → Logs Insights\nSelect log groups:\n/aws/lambda/daivietblood-get-users /aws/lambda/daivietblood-create-user /aws/lambda/daivietblood-emergency-requests Sample query - Find errors:\nfields @timestamp, @message | filter @message like /ERROR/ | sort @timestamp desc | limit 50 Sample query - Duration statistics: fields @timestamp, @duration | stats avg(@duration), max(@duration), min(@duration) by bin(1h) Part 3: Resource Cleanup ⚠️ Important: Follow these steps to avoid unexpected charges.\nCleanup Order (Important!) Clean up in the following order to avoid dependency errors:\nStep 1: Delete Amplify App Go to Amplify Console Select daivietblood-frontend Actions → Delete app Confirm deletion Step 2: Delete CloudFront Distribution Go to CloudFront Console Select distribution → Disable Wait for status to change to \u0026ldquo;Deployed\u0026rdquo; Select distribution → Delete Step 3: Delete S3 Buckets Go to S3 Console Select bucket daivietblood-assets-* Empty bucket first Then Delete bucket Step 4: Delete API Gateway Go to API Gateway Console Select daivietblood-api Actions → Delete Step 5: Delete Lambda Functions Go to Lambda Console Delete each function: daivietblood-get-users daivietblood-create-user daivietblood-emergency-requests Delete Lambda Layer: mysql2-layer Step 6: Delete RDS Instance Go to RDS Console → Databases Select daivietblood-db Actions → Delete Uncheck \u0026ldquo;Create final snapshot\u0026rdquo; Check \u0026ldquo;I acknowledge\u0026hellip;\u0026rdquo; Type delete me to confirm Step 7: Delete VPC Resources Go to VPC Console\nDelete NAT Gateway:\nNAT Gateways → Select NAT Gateway → Delete Wait for status \u0026ldquo;Deleted\u0026rdquo; Release Elastic IP:\nElastic IPs → Select EIP → Release Delete VPC Endpoints (if any):\nEndpoints → Select endpoints → Delete Delete Security Groups (except default):\nSecurity Groups → Delete daivietblood-lambda-sg, daivietblood-rds-sg Delete DB Subnet Group:\nRDS Console → Subnet groups → Delete daivietblood-db-subnet-group Delete VPC:\nYour VPCs → Select daivietblood-vpc → Delete VPC This will delete subnets, route tables, internet gateway Step 8: Delete CI/CD Resources CodePipeline Console → Delete daivietblood-pipeline CodeBuild Console → Delete daivietblood-backend-build Step 9: Delete CloudWatch Resources CloudWatch → Dashboards → Delete DaiVietBlood-Monitoring CloudWatch → Alarms → Delete all related alarms CloudWatch → Log groups → Delete log groups /aws/lambda/daivietblood-* Step 10: Delete IAM Resources IAM Console → Roles Delete roles: daivietblood-lambda-role codebuild-daivietblood-* codepipeline-daivietblood-* Cleanup Checklist Amplify app deleted CloudFront distribution deleted S3 buckets emptied and deleted API Gateway deleted Lambda functions and layers deleted RDS instance deleted NAT Gateway deleted Elastic IP released VPC and all components deleted CodePipeline and CodeBuild deleted CloudWatch dashboards, alarms, log groups deleted IAM roles deleted Verify No Remaining Charges Go to AWS Cost Explorer Verify no resources are running Go to Billing Console → Bills to confirm 💡 Tip: Set up Budget Alert in AWS Budgets to receive notifications when costs exceed threshold.\nWorkshop Conclusion Congratulations! 🎉 You have completed the workshop on building a Serverless system on AWS.\nWhat You Learned: ✅ Design and deploy VPC with Public/Private Subnets ✅ Create RDS MySQL in a secure environment ✅ Build Lambda functions and expose via API Gateway ✅ Configure S3 and CloudFront for static assets ✅ Deploy React app with AWS Amplify ✅ Set up automated CI/CD Pipeline ✅ Monitor application with CloudWatch Next Steps: Learn more about AWS Well-Architected Framework Explore advanced features like X-Ray tracing Experiment with Aurora Serverless for database Implement authentication with Amazon Cognito "},{"uri":"https://lhoanggg.github.io/intership-report/6-self-evaluation/","title":"Self-Assessment","tags":[],"description":"","content":"During my internship at Amazon Web Services Vietnam from 09/08/2025 to 11/28/2025, I had the opportunity to learn, practice, and apply knowledge about cloud computing, particularly AWS services, in a real working environment.\nI participated in backend development for an application using TypeScript, deploying Lambda Functions and API Gateway on AWS, through which I improved my skills in TypeScript programming, designing and deploying serverless backend, managing API Gateway, integrating Lambda with other AWS services, as well as writing technical reports and optimizing system performance.\nRegarding work attitude, I always strive to complete tasks efficiently, follow company regulations, and actively communicate with colleagues to enhance productivity in a DevOps and Cloud environment.\nTo objectively reflect on my internship experience, I would like to self-assess based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ☐ ✅ ☐ 2 Ability to learn Ability to absorb new knowledge and learn quickly ☐ ✅ ☐ 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ✅ ☐ ☐ 4 Sense of responsibility Completing tasks on time and ensuring quality ✅ ☐ ☐ 5 Discipline Adhering to schedules, rules, and work processes ✅ ☐ ☐ 6 Progressive mindset Willingness to receive feedback and improve oneself ✅ ☐ ☐ 7 Communication Presenting ideas and reporting work clearly ☐ ✅ ☐ 8 Teamwork Working effectively with colleagues and participating in teams ✅ ☐ ☐ 9 Professional conduct Respecting colleagues, partners, and the work environment ✅ ☐ ☐ 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ☐ ✅ ☐ 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ✅ ☐ ☐ 12 Overall General evaluation of the entire internship period ✅ ☐ ☐ Needs Improvement Further develop the ability to learn and absorb knowledge, as well as the thinking skills to analyze and solve problems that arise in work.\nImprove daily and professional communication skills and handling of situations effectively.\n"},{"uri":"https://lhoanggg.github.io/intership-report/7-feedback/","title":"Sharing and Feedback","tags":[],"description":"","content":" Here, you can freely share your personal opinions about your experience participating in the First Cloud Journey program. This will help the FCJ team improve any shortcomings based on the following aspects:\nOverall Evaluation 1. Working Environment\nThe working environment at FCJ is very friendly and open. Colleagues and mentors are always willing to support me whenever I encounter difficulties, even outside working hours. The workspace is clean and comfortable, which helps me stay focused and productive. However, I think having a larger office area would provide interns with more opportunities to work on-site.\n2. Support from Mentor / Team Admin\nMy mentor provided detailed guidance and clear explanations whenever I struggled to understand an issue, and always encouraged me to ask questions. The admin team offered full support, from sharing knowledge and documents to creating opportunities for me to participate in events. I truly appreciate that the mentors and admin team allowed me to experiment and solve problems on my own instead of simply giving me the answers.\n3. Relevance of Work to Academic Major\nThe tasks assigned to me aligned well with the knowledge I learned in my major, while also exposing me to new areas that I had never explored before. This helped me reinforce my foundational knowledge while gaining practical skills.\n4. Learning \u0026amp; Skill Development Opportunities\nDuring the internship, I had the opportunity to work with many new technologies, especially cloud-related services and tools. In addition, I learned more about project management, teamwork, and professional communication. The mentors also shared valuable real-world experience, giving me clearer direction for my career path.\n5. Company Culture \u0026amp; Team Spirit\nThe company culture is very positive: people respect one another, work seriously yet maintain a cheerful atmosphere. This made me feel like a real part of the team, even as an intern.\n6. Internship Policies / Benefits\nThe company provided flexible working hours when needed. Moreover, being able to participate in workshops and networking activities was a great advantage.\nAdditional Questions What I am most satisfied with is the opportunity to work directly with AWS services and experience the professional environment at Amazon Web Services Vietnam. The mentors guided me enthusiastically and allowed me to experiment and solve problems myself instead of giving me ready-made answers. Thanks to this, I learned a lot of practical knowledge about cloud computing and serverless development.\nI think the company could expand the office space so interns have more chances to work directly on-site. Having more workspace would improve communication, learning, and interaction with mentors and team members.\nAbsolutely yes, I would recommend this internship to my friends. AWS Vietnam is an ideal environment for those who want to develop cloud-related skills, especially in Software Engineering or DevOps. You get to work with modern technologies, learn from experienced mentors, and experience an open, proactive, and creativity-driven culture.\nSuggestions \u0026amp; Expectations I truly hope to continue joining future programs or opportunities from AWS, as this environment helps me learn more about cloud technologies and grow in the right direction for my career.\nI would like to express my sincere gratitude to AWS Vietnam—especially to Master Hưng, Mr. Kha, and the entire FCJ admin/mentor team—for providing such a professional, supportive, and open internship environment. This has been a valuable experience that helped me grow in both technical skills and professional work ethic.\n"},{"uri":"https://lhoanggg.github.io/intership-report/4-eventparticipated/4.6-event6/","title":"","tags":[],"description":"","content":"Here is the English version of the report, maintaining the structure and formatting suitable for your AWS program submission.\ntitle: \u0026ldquo;Event 6\u0026rdquo; date: \u0026ldquo;2025-09-08\u0026rdquo; weight: 1 chapter: false pre: \u0026quot; 4.6. \u0026quot;\nEvent Summary Report: AWS Cloud Mastery Series #3 Topic: AWS Well-Architected – Security Pillar Workshop\n1. Overview \u0026amp; Speakers The event focused on the most critical pillar within the AWS Well-Architected Framework: Security. The content was designed to equip attendees with knowledge ranging from fundamental to advanced levels regarding identity, monitoring, infrastructure protection, data protection, and incident response processes.\nSpeakers \u0026amp; Experts: The event gathered experts from the AWS Community (AWS Community Builders), AWS Cloud Club Captains from various universities (HCMUTE, SGU, PTIT, HUFLIT), Cloud Engineers from FCJ, and specially featuring Mendel Grabski (Security \u0026amp; DevOps Expert).\nAbout AWS Cloud Club: This is a network connecting students and professionals, helping to develop technical leadership skills, providing hands-on experiences, and offering long-term mentoring opportunities. The participating Cloud Clubs under FCJA include: HCMUTE, SGU, PTIT, and HUFLIT.\n2. Key Technical Highlights A. Identity \u0026amp; Access Management (IAM) IAM is defined as the \u0026ldquo;first line of defense.\u0026rdquo; The session emphasized the shift from manual management to automation and strict adherence to key principles:\nLeast Privilege Principle: Grant only the necessary permissions required to perform a task. Root User Protection: Delete access keys immediately after creation. Service Control Policies (SCPs): Use Organization-level policies to set a \u0026ldquo;ceiling\u0026rdquo; (maximum available permissions) for member accounts (Note: SCPs only filter permissions; they do not grant them). Permission Boundaries: Set the maximum permissions that an identity-based policy can grant to a specific User/Role. Multi-Factor Authentication (MFA): Encouraged the use of FIDO2 (hardware keys/biometrics) over traditional TOTP. Credential Rotation: Use AWS Secrets Manager to automate the rotation process (create -\u0026gt; set -\u0026gt; test -\u0026gt; finish) and integrate with EventBridge to manage schedules, eliminating risks associated with \u0026ldquo;hardcoded credentials.\u0026rdquo; B. Continuous Monitoring \u0026amp; Threat Detection The focus was on building comprehensive visibility and automated response capabilities:\nMulti-Layer Monitoring: Combining CloudTrail (recording API calls, S3/Lambda access) and VPC Flow Logs (network traffic). Event-Driven Security: Using EventBridge as a central event bus to route real-time alerts to Lambda/SNS/SQS or coordinate actions across different accounts (Cross-account routing). Detection-as-Code: Managing detection rules and queries (CloudTrail Lake queries) as code (version control) to ensure consistent deployment across the organization. Deep Dive into Amazon GuardDuty: This is an intelligent threat detection solution that operates continuously based on three main data sources: CloudTrail, VPC Flow Logs, and DNS Logs.\nExpanded Coverage: Protection for S3, EKS, RDS (brute-force detection), Lambda (suspicious network activity), and Malware Protection (EBS scanning). Runtime Monitoring: Uses an Agent to monitor deep inside the OS (processes, file access) on EC2/EKS/Fargate. C. Compliance \u0026amp; Infrastructure as Code (IaC) Security compliance is no longer a manual check but is integrated into the deployment pipeline:\nApplied Standards: AWS Foundational Security Best Practices, CIS Benchmark, PCI DSS, NIST. Enforcement Mechanism: Using AWS CloudFormation (IaC) to deploy standard configurations, combined with AWS Security Hub to automatically audit resources against defined standards. D. Network \u0026amp; Data Protection Network Security: Clearly distinguishing between Security Groups (Stateful - instance level firewall) and NACLs (Stateless - subnet level firewall). Introduction to AWS Network Firewall for advanced Egress filtering/IPS and integration with Threat Intelligence to automatically block malicious traffic. Data Protection: Encryption: Using KMS with Customer Master Keys (CMK) and Policy conditions to control decryption contexts. Certificates: Using AWS ACM to manage and automatically renew SSL/TLS certificates. Service Security: Enforcing HTTPS/TLS 1.2+ for S3 (via Bucket Policy) and Databases (e.g., PostgreSQL rds.force_ssl=1). E. Incident Response (IR) The standard IR process consists of 5 steps: Preparation -\u0026gt; Detection \u0026amp; Analysis -\u0026gt; Containment -\u0026gt; Eradication \u0026amp; Recovery -\u0026gt; Post-Incident Activity.\nPrevention Strategy: Never make S3 buckets public, isolate sensitive services in private subnets, and ensure all infrastructure changes go through IaC with a review process (double-gate). 3. Practical Experience \u0026amp; Q\u0026amp;A The event provided high practical value, specifically aligning with the \u0026ldquo;Automated Incident Response and Forensics\u0026rdquo; project our team is developing.\nDiscussion Point: During our project testing, the team noticed that Amazon GuardDuty has a latency of about 5 minutes to generate a finding after an incident occurs. We asked about solutions to reduce this latency.\nExpert Answer:\nThe Nature of the Service: The GuardDuty latency is an accepted technical characteristic because the system needs time to analyze large datasets to accurately determine threats and avoid false positives. Alternative Solutions: To achieve near real-time detection, the expert suggested integrating 3rd party solutions like OpenClarity (open source) or building custom anomaly detection logic based on CloudTrail events. Networking: After the event, Mr. Mendel Grabski (Ex-Head of Security \u0026amp; DevOps) expressed interest and offered professional support for our project, opening up valuable collaboration and mentoring opportunities.\nSome photos from the event participation [Add your photos here] Overall, the event not only provided technical knowledge but also changed my mindset regarding application design, system modernization, and effective team collaboration.\n"},{"uri":"https://lhoanggg.github.io/intership-report/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://lhoanggg.github.io/intership-report/tags/","title":"Tags","tags":[],"description":"","content":""}]